\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}                       % custom enum labels
\usepackage{parskip}          
\usepackage{physics}
\usepackage{geometry} 
\usepackage{esint}
\geometry{
  paperwidth=18cm,
  left=8mm,
  right=8mm,
  top=8mm,
  bottom=8mm,
}
\usepackage[framemethod=TikZ]{mdframed}     % graphics and framed envs

\renewcommand{\familydefault}{\sfdefault}      % sans serifs text
\setlength{\parindent}{0pt}                    % no paragraph indentation

% region TITLE CONSTRUCTION
\newlength\mywidth
\mywidth=\wd0
\renewcommand{\contentsname}{\hangindent=\mywidth \courseid: \coursetitle \\ \medskip \LARGE{[MY NAME HERE], \semester}}
% endregion

% region FRAMED ENVIRONMENTS
\newcounter{chapter}\setcounter{chapter}{1}
\newcounter{theo}[chapter]\setcounter{theo}{0}
\newcommand{\numTheo}{\arabic{chapter}.\arabic{theo}}
\newcommand{\mdftheo}[3]{
    \mdfsetup{
        frametitle={
            \tikz[baseline=(current bounding box.east),outer sep=0pt]
            \node[anchor=east,rectangle,fill=#3]
            {\ifstrempty{#2}{\strut #1~\numTheo}{\strut #1~\numTheo:~#2}};
        },
        innertopmargin=4pt,linecolor=#3,linewidth=2pt,
        frametitleaboveskip=\dimexpr-\ht\strutbox\relax,
        skipabove=11pt,skipbelow=0pt
    }
}
\newcommand{\mdfnontheo}[3]{
    \mdfsetup{
        frametitle={
            \tikz[baseline=(current bounding box.east),outer sep=0pt]
            \node[anchor=east,rectangle,fill=#3]
            {\ifstrempty{#2}{\strut #1}{\strut #1:~#2}};
        },
        innertopmargin=4pt,linecolor=#3,linewidth=2pt,
        frametitleaboveskip=\dimexpr-\ht\strutbox\relax,
        skipabove=11pt,skipbelow=0pt
    }
}
\newcommand{\mdfproof}[1]{
    \mdfsetup{
        skipabove=11pt,skipbelow=0pt,
        innertopmargin=4pt,innerbottommargin=4pt,
        topline=false,rightline=false,
        linecolor=#1,linewidth=2pt
    }
}
\newenvironment{theorem}[1][]{
    \refstepcounter{theo}
    \mdftheo{Theorem}{#1}{red!25}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{lemma}[1][]{
    \refstepcounter{theo}
    \mdftheo{Lemma}{#1}{red!15}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{corollary}[1][]{
    \refstepcounter{theo}
    \mdftheo{Corollary}{#1}{red!15}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{definition}[1][]{
    \mdfnontheo{Definition}{#1}{blue!20}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{example}[1][]{
    \mdfnontheo{Example}{#1}{yellow!40}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{proof}[1][]{
    \mdfproof{black!15}
    \begin{mdframed}[]\relax
\textit{Proof. }}{\end{mdframed}}
% endregion

% region NEW COMMANDS
\newcommand{\ds}{\displaystyle}
\newcommand{\pfn}[1]{\textrm{#1}}  % enables new functions
\newcommand{\mbf}[1]{\mathbf{#1}}  % mathbf
\newcommand{\C}{\mathbb{C}}        % fancy C
\newcommand{\R}{\mathbb{R}}        % fancy R
\newcommand{\Q}{\mathbb{Q}}        % fancy Q
\newcommand{\Z}{\mathbb{Z}}        % fancy Z
\newcommand{\N}{\mathbb{N}}   
\newcommand{\K}{\mathbb{K}}  % fancy N
\newcommand{\V}{\mathbf{V}} %vector space 
\newcommand{\0}{\mathbf{0}} %zero vector 
\newcommand{\from}{\leftarrow}
\renewcommand{\i}[1]{\textit{#1}}
\renewcommand{\b}[1]{\textbf{#1}}
\newcommand{\qed}{$\square$}
\newcommand{\range}{\text{range }}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}

% endregion

\title{Linear Algebra II}
\author{Hrishikesh Belagali}
\date{August 2025}
\begin{document}
\maketitle
\section{Basics}
\begin{example}
    For a set $\mathcal S$, let $\mathbb F^{\mathcal S}$ be 
    the set of all functions from $\mathcal S$ to $\mathbb F$.
    Then, defined over canonical addition and scalar
    multiplications, $\mathbb F^{\mathcal S}$ is a vector
    space. The additive identity is the zero function $0$,
    defined as $0(x) = 0$. The additive inverse can be defined
    as $-f : \mathcal S \to \mathbb F$ defined as $-f(x) = 
    -(f(x)) \forall x \in \mathcal S$. \\
    
    Note that $\mathbb F^n$ and $\mathbb F^{\infty}$ are 
    special cases of $\mathbb F^{\mathcal S}$, 
    where $\mathcal S$ is a finite set of size $n$ or 
    an infinite set, respectively.
\end{example}

Note that the empty set $\phi$ is not a vector space, nor is it a subspace of any vector space.
\begin{example}
The set of differentiable real-valued functions is a subspace of $\mathbb R^{\mathbb R}$. Note that in calculus, the sum of two continuous functions 
is continuous, and the sum of two differentiable functions is differentiable. Also, scalar multiples of continuous and differentiable functions are continuous and differentiable, respectively.
\end{example}

\begin{definition}
    Let $V_1, \cdots V_n$ be subspaces of a vector space $\V$. Then, the sum of these subspaces is defined as 
    $$V_1 + V_2 + \cdots + V_n = \{ v_1 + v_2 + \cdots + v_n \mid v_i \in V_i \text{ for all } i \}$$
\end{definition}

\begin{example}
    Let $$V_1 = \left \{ (w, w, x, x) \in \mathbb F^4 | w, x \in \mathbb F \right \}$$
    $$V_2 = \left \{ (y, y, y, z) \in \mathbb F^4 | y, z \in \mathbb F \right \}$$
    Now, let $v_1 \in V_1$ and $v_2 \in V_2$. Then, we can write
    $$v_1 = (w_1, w_1, x_1, x_1)$$
    $$v_2 = (y_2, y_2, y_2, z_2)$$
    for some $w_1, x_1, y_2, z_2 \in \mathbb F$. Then, we have
    $$v_1 + v_2 = (w_1 + y_2, w_1 + y_2, x_1 + y_2, x_1 + z_2) \in V_1 + V_2$$
    Let $W$ be defined as 
    $$W = \left \{ (x, x, y, z) \in \mathbb F^4 | x, y, z, \in \mathbb F \right \}$$
    Then, $v_1 + v_2 \in W$ so $V_1 + V_2 \subseteq W$. \\
    Let $w \in W$. Then, we can write
    $$w = (x_w, x_w, y_w, z_w)$$
    for some $x_w, y_w, z_w \in \mathbb F$. Then, we have
    $$w = (x_w, x_w, y_w, z_w) = (x_w, x_w, y_w, y_w) + (0, 0, 0, z_w - y_w) \in V_1 + V_2$$
    $\therefore W = V_1 + V_2$
\end{example}

\begin{lemma}
    For any subspaces $V_1, \cdots V_n$ of a vector space $\V$, $V_1 + \cdots + V_n$ is a subspace of $\V$. It is also the smallest subspace of $V$
    that contains all elements of the form $v_1 + \cdots + v_n$ where $v_i \in V_i$ for all $i$.
\end{lemma}
\begin{proof}
    From the definition and that $V_1, \cdots V_n$ are subspaces,
    Since the subspaces themselves are closed under addition and scalar multiplication, $V_1 + \cdots + V_n$ is also closed under addition and scalar multiplication. Also, the zero vector $\0$ is in each of the subspaces, so $\0 \in V_1 + \cdots + V_n$. Thus, $V_1 + \cdots + V_n$ is a subspace of $\V$. 
\end{proof}

Note: Generally, the set theoretic union is rarely a subspace, except for trivial cases where one space is a subspace of the other. 
However, intersections of subspaces are generally subspaces.

\begin{definition}[Direct sum]
    Let $V_1, \cdots V_n$ be subspaces of a vector space $\V$. Then, the sum $V_1 + \cdots + V_n$ is called a direct sum if each element of $V_1 + \cdots + V_n$ can be written in one and only one way as $v_1 + \cdots + v_n$ where $v_i \in V_i$ for all $i$. 
    In this case, we say that the sum is a direct sum, denoted by 
    $$W = V_1 \oplus V_2 \oplus \cdots \oplus V_n$$
\end{definition}

\begin{example}
    Let $$U = \left \{ (x, x, y) \in \mathbb F^3 | x, y \in \mathbb F \right \}$$
    Let $$W = \left \{ (x, 0, 0) \in \mathbb F^3 | x \in \mathbb F \right \}$$
    Then, $U$ and $W$ are subspaces of $\mathbb F^3$.
    Any arbitrary vector in $\mathbb F^3$ can be written as
    $$\begin{pmatrix}
        a \\
        b \\
        c
    \end{pmatrix} = \begin{pmatrix}
        b \\
        b \\
        c
    \end{pmatrix} + \begin{pmatrix}
        a-b \\
        0 \\
        0
    \end{pmatrix}$$
    Since this is a unique representation, $U \oplus W = \mathbb F^3$.
\end{example}
\begin{theorem}
    Let $V_1, \cdots V_n$ be subspaces of a vector space $\V$. Then, $V_1  + \cdots V_n = V_1 \oplus \cdots \oplus V_n$ if and only if the only way to write $\0$ as $v_1 + \cdots + v_n$ where $v_i \in V_i$ for all $i$ is to take each $v_i = \0$.
    In other words, if $v_1 + \cdots + v_n = \0$ implies that each $v_i = \0$, then the sum is a direct sum.
\end{theorem}
\begin{proof}
    Suppose that $V_1 + \cdots V_n$ is a direct sum. Then, the additive identity can be written as 
    the sum of additive identities from each subspace. By definition of a direct sum, this is the \textit{only}
    way to write the additive identity as a sum. \\ 
    Suppose that the only way to write zero is as the sum 
    of additive identities from each subspace. Consider an arbitrary vector 
    $v \in V$. Suppose that there are two different ways of writing the sum,
    $$v = u_1 + \cdots u_n ; u_k \in V_k$$ 
    $$v = v_1 + \cdots v_n ; v_k \in V_k$$
    Then, we can subtract these two equations 
    $$0 = (u_1 - v_1) + \cdots + (u_n - v_n) ; (u_k - v_k) \in V_k$$
    Since the only way to write zero is as the sum of additive identities from each subspace, we must have $u_k - v_k = 0$ for all $k$. Thus, $u_k = v_k$ for all $k$, and the representation is unique.
    Therefore, by definition, the sum $V_1 + \cdots V_n$ is a direct sum.
\end{proof}

\begin{theorem}
    Let $U$ and $W$ be subspaces of a vector space $\V$. Then, the sum $U + W$ is a direct sum if and only if $U \cap W = \{0\}$.
\end{theorem}
\begin{proof}
    Suppose that $U + W$ is a direct sum. Let $v \in U \cap W$. Then, $v \in U$, and $-v \in W$
    $$0 = v + (-v)$$
    Since the representation is unique, we must have $v = 0$. Thus, $U \cap W = \{0\}$.

    Conversely, suppose that $U \cap W = \{0\}$. Let $u \in U$ and $w \in W$. Then, we can write
    $$u + w = 0$$
    From the previous result, it suffices to show that $u = w = 0$. This implies that $w$ is the additive inverse
    of $u$, meaning $u, w \in U \cap W = \{0\}$. Therefore, $u = w = 0$ and $U + W$ = $U \oplus W$.
\end{proof}
\begin{lemma}
    Suppose $v_1, \cdots v_m$ is a linearly dependent list in V. Then there exists $j \in \{1, \cdots m\}$ such that
    $$v_j \in \text{span}(v_1, \cdots, v_{j-1})$$
    If this condition holds, then $$\text{span} (v_1, \cdots v_m) = \text{span} (v_1, \cdots, v_{j-1}, v_{j+1}, \cdots v_m)$$
\end{lemma}
\begin{proof}
    Since $v_1, \cdots v_m$ is linearly dependent,
    there exist $a_1, \cdots a_m \in \mathbb F$ not all zero
    such that
    $$a_1v_1 + \cdots + a_mv_m = 0$$
    Let $k$ be the largest index such that $a_k \neq 0$.
    Then, $$v_k = -\frac{a_1}{a_k}v_1 - \cdots - \frac{a_{k-1}}{a_k}v_{k-1}$$
    Therefore, $v_k \in \text{span}(v_1, \cdots, v_{k-1})$. \\
    \\Suppose $v_k \in \text{span}(v_1, \cdots, v_{k-1})$. Then, we can write
    $$v_k = b_1v_1 + \cdots + b_{k-1}v_{k-1}$$
    for some $b_1, \cdots, b_{k-1} \in \mathbb F$.
    Let $u \in \text{span}(v_1, \cdots v_m)$. Then, 
    $$u = c_1v_1 + \cdots c_mv_m \text{ for some } c_1, \cdots c_m \in \mathbb F$$
    Substituting for $v_k$, we get
    $$u = c_1v_1 + \cdots + c_{k-1}v_{k-1} + c_k(b_1v_1 + \cdots + b_{k-1}v_{k-1}) + c_{k+1}v_{k+1} + \cdots + c_mv_m$$
    Therefore, 
    $u \in \text{span}(v_1, \cdots, v_{k-1}, v_{k+1}, \cdots v_m)$.
    Proving the other direction is trivial, you can set the coefficients of higher indices than $k$ to zero.
\end{proof}
\begin{theorem}
    Let $V$ be a finite-dimensional vector space. Suppose that 
    $u_1, \cdots u_m$ is linearly independent in $V$ and $w_1, \cdots w_n$.
    Then, $m \leq n$. In other words, any linearly independent list is smaller
    or the same size as any spanning list.
\end{theorem}

\begin{theorem}
    Suppose $U$ is a subspace of a finite-dimensional vector space $V$. Then there exists
    a subspace $W$ of $V$ such that $V = U \oplus W$.
\end{theorem}
\begin{proof}
    Let $u_1, \cdots u_m$ be a basis of $U$. Since $U$ is a subspace of $V$, we can extend this basis to a basis of $V$, say $u_1, \cdots u_m, w_1, \cdots w_n$. Let $W = \text{span}(w_1, \cdots w_n)$. Then, we have
    $$V = U + W$$
    since any vector in $V$ can be written as a linear combination of the basis vectors. \\
    Now, we need to show that the sum is direct. Suppose that 
    $$u + w = 0 ; u \in U, w \in W$$
    Then, we can write
    $$u = a_1u_1 + \cdots + a_mu_m ; a_i \in \mathbb F$$
    $$w = b_1w_1 + \cdots + b_nw_n ; b_i \in \mathbb F$$
    Therefore, we have
    $$a_1u_1 + \cdots + a_mu_m + b_1w_1 + \cdots + b_nw_n = 0$$
    Since the basis vectors are linearly independent, all coefficients must be zero.
    Thus, $u = w = 0$, and the sum is direct.
\end{proof}
\begin{theorem}
    If $V_1$ and $V_2$ are finite-dimensional subspaces of $V$,
    then 
    $$\text{dim}(V_1 + V_2) = \text{dim}(V_1) + \text{dim}(V_2) - \text{dim}(V_1 \cap V_2)$$
\end{theorem}
\begin{proof}
    Let $u_1, \cdots u_m$ be a basis of $V_1 \cap V_2$. We can extend this basis to a basis of $V_1$, say $u_1, \cdots u_m, v_1, \cdots v_k$. Similarly, we can extend the basis of $V_1 \cap V_2$ to a basis of $V_2$, say $u_1, \cdots u_m, w_1, \cdots w_l$. We claim that the list 
    $$u_1, \cdots u_m, v_1, \cdots v_k, w_1, \cdots w_l$$
    is a basis of $V_1 + V_2$. \\
    Since the dimension of $V_1$ is $m+k$ and the dimension of $V_2$ is $m+l$, the dimension of $V_1 + V_2$ is at most $m+k+l$
    Therefore,
    $$\dim (V_1 + V_2) = m + k + l = (m+k) + (m+l) - m = \dim(V_1) + \dim(V_2) - \dim(V_1 \cap V_2)$$
    It remains to show that the list spans $V_1 + V_2$ and is linearly independent. \\
    Let $v \in V_1 + V_2$. Then, we can write 
    $$v = v_1 + v_2 ; v_1 \in V_1, v_2 \in V_2$$
    Since $u_1, \cdots u_m, v_1, \cdots v_k$ is a basis of $V_1$, we can write
    $$v_1 = a_1u_1 + \cdots + a_mu_m + b_1v_1 + \cdots + b_kv_k ; a_i, b_i \in \mathbb F$$
    Similarly, we can write
    $$v_2 = c_1u_1 + \cdots + c_mu_m + d_1v_1 + \cdots + d_kv_k ; c_i, d_i \in \mathbb F$$  
    Therefore, we have
    $$v = (a_1 + c_1)u_1 + \cdots + (a_m + c_m)u_m + b_1v_1 + \cdots + b_kv_k + d_1w_1 + \cdots + d_lw_l$$
    Thus, the list spans $V_1 + V_2$. \\
    To show that the list is linearly independent, suppose we have a linear combination
    $$\sum_{i=1}^m \alpha_i u_i + \sum_{j=1}^k \beta_j v_j + \sum_{l=1}^l \gamma_l w_l = 0$$
    for some scalars $\alpha_i, \beta_j, \gamma_l \in \mathbb F$. We need to show that all coefficients must be zero. \\
    Since $u_1, \cdots u_m, v_1, \cdots v_k$ is a basis of $V_1$, we can write
    $$v_1 = a_1u_1 + \cdots + a_mu_m + b_1v_1 + \cdots + b_kv_k ; a_i, b_i \in \mathbb F$$
    Similarly, we can write
    $$v_2 = c_1u_1 + \cdots + c_mu_m + d_1v_1 + \cdots + d_kv_k ; c_i, d_i \in \mathbb F$$
    Complete ts later
\end{proof}
\section{Eigenvectors and eigenvalues}
\subsection{Invariant subspaces}
\begin{definition}[Invariant subspace]
    Let $T \in \mathcal L(\V)$. A subspace $U$ of $\V$ is called invariant under $T$ if 
    $$u \in U \implies T(u) \in U$$
    In other words, $U$ is invariant under $T$ if $T(U) \subseteq U$.
\end{definition}
\begin{definition}[Eigenvalues and eigenvectors]
    A number $\lambda \in \mathbb F$ is called an eigenvalue of $T \in \mathcal L(\V)$ if there exists a non-zero vector $v \in \V$ such that
    $$T(v) = \lambda v$$
    Such a vector $v$ is called an eigenvector corresponding to the eigenvalue $\lambda$.
\end{definition}
\begin{definition}[Polynomials of linear operators]
    Let $m$ be a positive integer. Define $T^m$ as $T \circ T \circ \cdots \circ T$ ($m$ times). Define $T^0 = I$. Define $T^{-m} = (T^{-1})^m$ if $T$ is invertible.
    Then, 
    let $p(z) = a_mz^m + a_{m-1}z^{m-1} + \cdots + a_1z + a_0$ be a polynomial with coefficients in $\mathbb F$. Then, we can define the polynomial of the operator $T$ as
    $$p(T) = a_mT^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I$$
\end{definition}
\begin{lemma}
If $p$, $q$ are polynomials and $T \in \mathcal L(\mathbf V)$, then $(pq)(T) = p(T)q(T) = q(T)p(T)$.
\end{lemma}
\begin{proof}
    Let $p(z) = a_mz^m + a_{m-1}z^{m-1} + \cdots + a_1z + a_0$ and $q(z) = b_nz^n + b_{n-1}z^{n-1} + \cdots + b_1z + b_0$. Then, we have
    $$p(T) = a_mT^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I$$
    $$q(T) = b_nT^n + b_{n-1}T^{n-1} + \cdots + b_1T + b_0I$$
    Now, we can compute the product $p(T)q(T)$ as follows:
    \begin{align*}
        p(T)q(T) &= (a_mT^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I)(b_nT^n + b_{n-1}T^{n-1} + \cdots + b_1T + b_0I) \\
        &= a_mb_nT^{m+n} + (a_mb_{n-1} + a_{m-1}b_n)T^{m+n-1} + \cdots + (a_0b_0)I
    \end{align*}
    This is exactly the polynomial $(pq)(T)$, where $(pq)(z) = p(z)q(z)$.
    Since multiplication of polynomials is commutative, we also have $p(T)q(T) = q(T)p(T)$.
    \end{proof}
    \begin{theorem}
    Let $T \in \mathcal L(\V)$ and $p \in \mathcal P(\mathbb F)$. Then, $\text{null } p(T)$ and $\text{range } p(T)$ are invariant under $T$.
    \end{theorem}
    \begin{proof}
        Let $u \in \text{null } p(T)$. Then, we have
        $$p(T)(u) = 0$$
        Applying $T$ to both sides, we get
        $$p(T)(Tu) = (p(T)T)u = (Tp(T))u = T(p(T)u) = T(0) = 0$$
        Thus, $T(u) \in \text{null } p(T)$, showing that $\text{null } p(T)$ is invariant under $T$.

        Now, let $v \in \text{range } p(T)$. Then, we can write
        $$v = p(T)(w)$$
        for some $w \in \V$. Applying $T$, we get
        $$T(v) = T(p(T)(w)) = p(T)(T(w))$$
        Thus, $T(v) \in \text{range } p(T)$, showing that $\text{range } p(T)$ is invariant under $T$.
    \end{proof}
    \subsection{Characteristic polynomials}
    \begin{definition}[Monic polynomial]
        A polynomial $p \in \mathcal P(\mathbb F)$ is called monic if the leading coefficient is 1.
    \end{definition}
    \begin{definition}[Minimal polynomial]
        Let $T \in \mathcal L(\V)$. A monic polynomial $p$ of smallest degree such that $p(T) = 0$ is called the minimal polynomial of $T$.
    \end{definition}
    \begin{theorem}
        Let $\mathbf V$ be finite dimensional. Let $T \in \mathcal L(\mathbf V)$. Then, there is a 
        unique monic polynomial $p$ in $\mathcal P(\mathbb F)$ of smallest degree such that $p(T) = 0$.
        Furthermore, the degree of $\deg p \leq \dim \mathbf V$.
    \end{theorem}
    \begin{proof}\\\\
        \textbf{Existence:} We will prove it by induction on the dimension of $\V$. \\\\
        \textit{Base case:} \\ If $\dim \V = 0$, then $\V = \{0\}$. Thus, $T$ is the zero operator, and we can take $p(z) = 1$. \\\\
        \textit{Inductive step:} \\
        Suppose that $\dim \V > 0$ and suppose that the results is true for all operators
        on all vector spaces of dimension strictly less than $\dim \V$. Let $u \in \V, u \neq 0$
        The list $u, Tu, T^2u, \cdots T^nu$ must be linearly dependent for some $n \leq \dim \V$.
        Then there exists a smallest positive integer $m$ such that
        $T^mu$ is a linear combination of the previous $T^ku$'s. Thus, we can write
        $$T^mu = -a_{m-1}T^{m-1}u - \cdots - a_1Tu - a_0u$$
        for some $a_0, \cdots a_{m-1} \in \mathbb F$. Let
        $$p_1(z) = z^m + a_{m-1}z^{m-1} + \cdots + a_1z + a_0$$
        Then, we have $p_1(T)(u) = 0$. \\
        Since $\text{null } p_1(T)$ is invariant under $T$, $u, Tu, T^2u, \cdots T^{m-1}u \in \text{null } p_1(T)$.
        Note that from the definition of $m$, the list $u, Tu, T^2u, \cdots T^{m-1}u$ is linearly independent.
        Therefore, $\dim \text{null } p_1(T) \geq m$. Then, 
        $$\dim \text{range } p_1(T) = \dim \V - \dim \text{null } p_1(T) \leq \dim \V - m < \dim \V \leq n - m$$
        Since $\text{range } p_1(T)$ is invariant under $T$, we can apply the inductive hypothesis to the operator
        $T|_{\text{range } p_1(T)}$. Thus, there exists a unique monic polynomial $q \in \mathcal P(\mathbb F)$ of degree $\leq n - m$
        such that $q(T|_{\text{range } p_1(T)}) = 0$. Then, for all $v \in \V$, we have 
        $p_1(T) \in \text{ range}q(T)$. Thus, $q(T)(p_1(T)(v)) = 0$. Therefore, we have
        $$(q p_1)(T)(v) = q(T)(p_1(T)(v)) = 0$$
        for all $v \in \V$. Therefore, $(qp_1)(T) = 0$, and we can take $p = qp_1$ as our minimal polynomial.
        
        \textbf{Uniqueness:} Suppose $p$ and $q$ are both monic polynomials of smallest degree
        such that $p(T) = q(T) = 0$. Then $(p-q)(T) = 0$, and since $p$ and $q$ have the
        same degree and leading coefficient, $p-q$ has degree strictly less than $\deg p$. Note that 
        $p-q$ is not necessarily monic. However, we can write $p-q = c r$ where $c$ is the leading coefficient of $p-q$ and $r$ is a monic polynomial. Then, we have
        $$0 = (p-q)(T) = c r(T)$$
        By minimality of $\deg p$, we must have $p-q = 0$, so $p = q$.
    \end{proof}
\begin{definition}
    Let $\V$ be finite-dimensional and $T \in \mathcal L(\V)$. Let $p(z)$ be the 
    minimal polynomial of $T$. Then, 
    \begin{itemize}
        \item The zeros of $p$ are the eigenvalues of $T$.
        \item If $\V$ is a complex vector space, then 
        $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
        where $\lambda_1, \cdots \lambda_m$ are the eigenvalues of $T$ (possibly repeated).
    \end{itemize}
\end{definition}
\begin{proof}
    \textbf{Forward: If $\lambda$ is a zero of $p$, then $\lambda$ is an eigenvalue of $T$.} \\
    Suppose that $p(\lambda) = 0$. Then, we can write
    $$p(z) = (z - \lambda)q(z)$$
    where $q(z)$ is a monic polynomial.
    Then, we have 
    $$p(T) = (T - \lambda I)q(T) = 0$$
    For some $v \in \V$ such that $q(T)v \neq 0$, we have 
    $$(T - \lambda I)(q(T)v) = 0$$
    Since $\deg{q} = \deg{p} - 1 < \deg{p}$, such a $v$ exists.
    Thus, $T(q(T)v) = \lambda (q(T)v)$, and $q(T)v$ is an eigenvector corresponding to the eigenvalue $\lambda$. \\\\
    \textbf{Reverse: If $\lambda$ is an eigenvalue of $T$, then $\lambda$ is a zero of $p$.} \\
    Suppose that $\lambda$ is an eigenvalue of $T$. Then, there exists a non-zero vector $v \in \V$ such that
    $$T(v) = \lambda v$$
    Then, we have
    $$T(Tv) = T(\lambda v) = \lambda T(v) = \lambda^2 v$$
    Continuing this way, we can show that
    $$T^k(v) = \lambda^k v$$
    for all non-negative integers $k$. Then, we have
    $$p(T)(v) = (T^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I)(v) = ( \lambda^m + a_{m-1}\lambda^{m-1} + \cdots + a_1\lambda + a_0)v = p(\lambda)v$$
    Since $p(T) = 0$, we have $p(\lambda)v = 0$. Since $v \neq 0$, we must have $p(\lambda) = 0$.

    By the fundamental theorem of algebra, if $\V$ is a complex vector space, then $p(z)$ can be factored into linear factors. Since the zeros of $p$ are the eigenvalues of $T$, we can write
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are the eigenvalues of $T$ (possibly repeated).
\end{proof}
\begin{corollary}
    Let $\V$ be a nonzero, finite-dimensional, complex vector space and $T \in \mathcal L(\V)$.
    Then, $T$ has an eigenvalue.
\end{corollary}
Note: The theorem is not true for infinite-dimensional vector spaces.
For example, consider the vector space $\mathbb C^{\infty}$ and the right shift operator $T$ defined as
$$T(x_1, x_2, x_3, \cdots) = (0, x_1, x_2, x_3, \cdots)$$
Then, $T$ has no eigenvalues. To see this, suppose that $\lambda$ is an eigenvalue of $T$. Then, there exists a non-zero vector $v = (x_1, x_2, x_3, \cdots)$ such that
$$T(v) = \lambda v$$
This gives us the equation
$$(0, x_1, x_2, x_3, \cdots) = \lambda (x_1, x_2, x_3, \cdots)$$
Equating components, we find that
$$0 = \lambda x_1$$
$$x_1 = \lambda x_2$$
$$x_2 = \lambda x_3$$
$$\vdots$$
If $\lambda \neq 0$, this implies that $x_1 = x_2 = x_3 = \cdots = 0$, contradicting the assumption that $v$ is non-zero. Therefore, we must have $\lambda = 0$. However, if $\lambda = 0$, then we have
$$T(v) = (0, x_1, x_2, x_3, \cdots) = 0$$
which again implies that $v = 0$. Thus, we conclude that $T$ has no eigenvalues. \\\\ 
Recall that $q(z)$ divides $p(z)$ if there exists a polynomial $r(z)$ such that $p(z) = q(z)r(z)$.
\begin{theorem}
    Let $\V$ be finite-dimensional and $T \in \mathcal L(\V)$.
    Let $p(z)$ be the minimal polynomial of $T$.
    Let $q(z) \in \mathcal P(\mathbb F)$.  Then, $q(T) = 0$ if and only if $p(z)$ divides $q(z)$.
\end{theorem}
\begin{proof}
    If $q = ps$ for some $s \in \mathcal P(\mathbb F)$, then
    $q(T) = p(T)s(T) = 0$, $s(T) = 0$. \\
    Conversely, suppose that $q(T) = 0$. Using polynomial long division, we can write
    $$q(z) = p(z)s(z) + r(z)$$
    where $\deg r < \deg p$. Then, we have
    $$r(T) = q(T) - p(T)s(T) = 0 - 0 = 0$$
    Since $\deg r < \deg p$, we must have $r(z) = 0$. Thus, we conclude that $p(z)$ divides $q(z)$.
\end{proof}
\begin{corollary}
    Let $\V$ be finite-dimensional and $T \in \mathcal L(\V)$.
    and let $U \subseteq \V$ be invariant under $T$.
    Let $T_U$ be the restriction of $T$ to $U$.
    Thus, the minimal polynomial of $T_U$ divides the minimal polynomial of $T$.
\end{corollary}

\begin{theorem}
    Let $\V$ be finite dimensional and $T \in \mathcal L(\V)$.
    Then, $T$ is not invertible if and only if the minimal polynomial of $T$ does not have a constant term.
\end{theorem}
\begin{proof}
    Let $p$ be the minimal polynomial of $T$.
    Suppose that $T$ is not invertible. Then,
    $\text {null }{T} \neq \{0\}$. Then, 0 is an eigenvalue of T.
    Thus, $p(0) = 0$, so $p$ does not have a constant term.
    Conversely, suppose that $p$ does not have a constant term.
    Then, 0 is an eigenvalue of $T$, so $\text {null }{T} \neq \{0\}$.
    Thus, $T$ is not invertible.
\end{proof}
\begin{theorem}
    Every operator on an odd-dimensional real vector space has an eigenvalue.
\end{theorem}
\begin{proof} 
    later
\end{proof}
\subsection{Upper-triangular matrices}
When discussing operators on $\V$, we'll fix a basis for $\V$.
Then, we can represent the operator as a matrix.
\begin{definition}[Upper triangular matrix]
    Let $T \in \mathcal L(\V)$, where $\V$ is finite-dimensional. The matrix of $T$
    with respect to a basis $v_1, \cdots v_n$ of $\V$ is called upper-triangular if
    $$Tv_j \in \text{span}(v_1, \cdots v_j)$$
    for all $j = 1, \cdots n$. In other words, the matrix of $T$ has all entries below the main diagonal equal to zero. 
\end{definition}
\begin{theorem}
    Let $T \in \mathcal{L}(\V)$, where $\V$ is finite dimensional.
    Let $v_1, \cdots v_n$ be a basis of $\V$.
    Then, the following are equivalent:
    \begin{enumerate}
    \item The matrix of $T$ with respect to $v_1, \cdots v_n$ is upper-triangular. 
    \item $span(v_1, \cdots v_j)$ is invariant under $T$ for $j = 1, \cdots n$.
    \item $Tv_j \in \text{span}(v_1, \cdots v_{j-1})$ for $j = 1, \cdots n$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    $(1) \implies (2)$: \\ 
    Suppose $k \in \{1, \cdots n\}$ and let $j \in \{1, \cdots k\}$. Then, we can write
    $$Tv_j = a_1v_1 + \cdots + a_jv_j$$
    for some $a_1, \cdots a_j \in \mathbb F$.
    Therefore, $Tv_j \in \text{span}(v_1, \cdots v_k)$.
    Since $j$ was arbitrary, we conclude that $\text{span}(v_1, \cdots v_k)$ is invariant under $T$. \\\\
    $(2) \implies (3)$: \\
    From the definition of $\text{span}(v_1, \cdots v_j)$ being invariant under $T$, we have
    $$Tv_k \in \text{span}(v_1, \cdots v_k)$$
    $(3) \implies (1)$: \\
    Suppose $k \in \{1, \cdots n\}$ and let $j \in \{1, \cdots k\}$. Then, we can write
    $$Tv_j = a_1v_1 + \cdots + a_jv_j$$
    for some $a_1, \cdots a_j \in \mathbb F$.
    Therefore, $Tv_j \in \text{span}(v_1, \cdots v_k)$.
    Since $j$ was arbitrary, we conclude that $\text{span}(v_1, \cdots v_k)$ is invariant under $T$. \\
\end{proof}
\begin{theorem}
    Let $T \in \mathcal{L}(\V)$, 
    where $\V$ is finite dimensional.
    If $\mathcal{M}(T)$ is upper-triangular with respect to some basis of $\V$,
    and $\lambda_1, \cdots \lambda_n$ are the entries on the main diagonal of $\mathcal{M}(T)$,
    then 
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_n I) = 0$$
\end{theorem}
\begin{proof}
    Let $p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_n)$.
    It suffices to show that $(T- \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_n I)$ vanishes
    on $\text{span} (v_1, \cdots v_k)$ for all $k= 1, \cdots n$. For $k=1$, 
    this is true since $Tv_1 = \lambda_1 v_1$. \\
    Now note $(T - \lambda_2 I) \in \text{span}(v_1)$, so 
    $(T - \lambda_1 I)(T - \lambda_2 I)v_2 = 0$.
    Continuing this way, we can show that
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_k I)v_k = 0$$
    for all $k = 1, \cdots n$. Thus, we conclude that
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_n I) = 0$$
\end{proof}
\begin{theorem}
    Let $T \in \mathcal L(\V)$, where $\V$ is finite-dimensional.
   Then, the eigenvalues are precisely the entries on the main diagonal of any upper-triangular matrix representing $T$.
\end{theorem}
\begin{proof}
    Let $p(z)$ be the minimal polynomial of $T$.
    Let $q(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_n)$,
    where $\lambda_1, \cdots \lambda_n$ are the entries on the main diagonal of the upper-triangular matrix representing $T$.
    Then, $q(T) = 0$. Thus, $p$ divides $q$. Then,
    $p(z)$ must be of the form 
    $$p(z) = (z - \lambda_{i_1})(z - \lambda_{i_2}) \cdots (z - \lambda_{i_m})$$
    where $i_1, i_2, \cdots i_m$ are some indices in $\{1, 2, \cdots n\}$.
    \\ 
    Let $j \in I = \{1, 2, \cdots n\}$ be arbitrary. Since the $j$th 
    diagonal of a product of upper-triangular matrix is the 
    product of the $j$th diagonals of the matrices, the $j$th
    element of $p(T)$ is $$\prod_{i\in I} (\lambda_j - \lambda_i) = 0$$
    Thus, there exists $i \in I$ such that $\lambda_j - \lambda_i = 0$, or $\lambda_j = \lambda_i$.
    Therefore, $\lambda_j$ is an eigenvalue of $T$.
\end{proof}
\begin{theorem}
    Let $\V$ be a finite-dimensional vector space and $T \in \mathcal{L}(\V)$.
    Then, $T$ has an upper-triangular matrix with respect to 
    some basis of $\V$ if and only if 
    the minimal polynomial of $T$ has the form 
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_n)$$
    where $\lambda_1, \cdots \lambda_n$ are in $\mathbb F$ (not necessarily distinct).
\end{theorem}
Note: This is always true for $\mathbb C$, it becomes an issue for 
$\mathbb R$.
\begin{proof}
    First, suppose that the matrix of $T$ is upper-triangular
    with respect to some basis $\mathcal{B} = \{v_1, \cdots v_n \}$.
    Let $\alpha_1, \cdots \alpha_n$ be the 
    diagonal entries of the matrix of $T$ and
    $q(z) = (z - \alpha_1)(z - \alpha_2) \cdots (z - \alpha_n)$.
    Then, we have $q(T) = 0$. Let $p(z)$ be the minimal polynomial of $T$.
    Then, $p$ divides $q$, so $p(z)$ must be of the form
    $$p(z) = (z - \alpha_1)(z - \alpha_2) \cdots (z - \alpha_n).$$
    Conversely, suppose that the minimal polynomial of $T$ has the form
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are in $\mathbb F$.
    We prove the result by induction on $m = \deg p$. \\\\
    Base case: $m=1$ \\ 
    In this case, we have $p(z) = z - \lambda_1$. Thus, $T = \lambda_1 I$.
    Therefore, the matrix of $T$ with respect to any basis is upper-triangular. \\\\
    Inductive step: \\
    Suppose that the result holds for all $k < m$. Let 
    $U = \text{range }(T - \lambda_m I)$.
    Then, $U$ is invariant under $T$.
    Let $T_U$ be the restriction of $T$ to $U$.
    Then, the minimal polynomial of $T_U$ divides $p(z)/(z - \lambda_m)$.
    Then, the minimal polynomial of $T_U$ has the form
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_{m-1})$$
    By the inductive hypothesis, there exists a basis
    $u_1, \cdots u_m$ of $U$ 
    such that the matrix of $T_U$ 
    is upper triangular with respect to this basis.
    Then, for $k \in \{ 1, \cdots m\}$, we have 
    $$Tu_k = T|_u (u_k) \in \text{span }(u_1, \cdots u_k)$$
    Now, extend this basis of $U$ to a basis of $\V$, say
    $u_1, \cdots u_m, v_1, \cdots v_n$
    where $n = \dim \V - \dim U$.
    If $k \in \{1, \cdots n\}$, then we have
    $$Tv_k = (T - \lambda_m I)v_k + \lambda_m v_k \in \text{span }
    (u_1, \cdots u_m, v_1, \cdots v_k)$$
    Therefore, $T$ is upper-triangular with respect to the basis
    $u_1, \cdots u_m, v_1, \cdots v_n$.
\end{proof}
\begin{corollary}
    Let $\V$ be a finite dimensional complex vector space 
    and $T \in \mathcal{L}(\V)$.
    Then, $T$ has an upper-triangular matrix with respect to some basis of $\V$.
\end{corollary}
\begin{proof}
    It follows from the fundamental theorem of algebra.
\end{proof}

\begin{definition}[Diagonalizable]
Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$.
We say that $T$ is diagonalizable if 
$T$ has a diagonal matrix with respect to some basis of $\V$. 
\end{definition}
\begin{example}
    Define $T \in \mathcal{L}(\mathbb R^2)$ by
    $$T\pmqty{x, y} = \pmqty{41x + 7y, -20x + 74y}$$
    Then, the matrix of $T$ wrt the standard basis is
    $$\mathcal{M}(T) = \pmqty{41 & 7 \\ -20 & 74}$$
    With respect to the basis $v_1 = \pmqty{1 \\ 4}, v_2 = \pmqty{7 \\ 5}$,
    the matrix of $T$ is
    $$\mathcal{M}(T) = \pmqty{69 & 0 \\ 0 & 46}$$
    Therefore, $T$ is diagonalizable.
\end{example}

It is convenient to give a name to the set of all eigenvectors (and $\0$).
\begin{definition}[Eigenspace]
    Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$.
    Let $\lambda$ be an eigenvalue of $T$.
    The eigenspace corresponding to $\lambda$ is defined as
    $$E(\lambda, T) = \text{null }(T - \lambda I)$$
\end{definition}
Note that $T$ restricted to $E(\lambda, T)$ is just multiplication by $\lambda$.
\begin{theorem}
    If $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T \in \mathcal{L}(\V)$,
    then then the sum of the corresponding eigenspaces is a direct sum, and
    $\dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T) \leq \dim \V$.
\end{theorem}
\begin{proof}
    Suppose $v_1, \cdots v_m = 0$, where 
    $v_k \in E(\lambda_k, T)$ for $k = 1, \cdots m$.
    We proved before that eigenvectors 
    corresponding to distinct eigenvalues are linearly
    independent. Thus, $v_1 = \cdots = v_m = 0$.
    Therefore, the sum is direct.
    The dimension inequality follows 
    from the properties of direct sums.
\end{proof}
We will be developing a criterion for diagonalizability.
\begin{theorem}
    Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$.
    Let $\lambda_1, \cdots \lambda_m$ be the distinct eigenvalues of $T$.
    Then, the following are equivalent:
    \begin{enumerate}
        \item $T$ is diagonalizable.
        \item $\V$ has a basis consisting of eigenvectors of $T$.
        \item $\V = E(\lambda_1, T) \oplus E(\lambda_2, T) \oplus \cdots \oplus E(\lambda_m, T)$
        \item $\dim E(\lambda_1, T) + \dim E(\lambda_2, T) + \cdots + \dim E(\lambda_m, T) = \dim \V$
\end{enumerate}
\end{theorem}
\begin{proof}
    $(1) \iff (2)$: \\ 
    Note that $T$ has a diagonal matrix
    with respect to some basis $(v_1, \cdots v_n)$
    if and only if 
    $$Tv_j = \lambda_j v_j$$
    This means that the basis consists
    of eigenvectors of $T$. \\\\
    $(2) \iff (3)$: \\
    Since $\V$ has a basis of eigenvectors of $T$,
    every vector in $\V$ can be written
    as a linear combination of these eigenvectors.
    $v = v_1, \cdots v_n$ where
    $v_k \in E(\lambda_{i_k}, T)$ for some $i_k \in \{1, \cdots m\}$.
    Thus, $$\V = E(\lambda_1, T) + E(\lambda_2, T) + \cdots + E(\lambda_m, T)$$.
    $$\V = E(\lambda_1, T) \oplus E(\lambda_2, T) \oplus \cdots \oplus E(\lambda_m, T)$$ 
    $(3) \iff (4)$: \\
    This follows from the dimension formula for direct sums. \\\\
    $(4) \implies (2)$: \\
    Let $u_1, \cdots u_k$ be a basis of $E(\lambda_1, T)$,
    $v_1, \cdots v_l$ be a basis of $E(\lambda_2, T)$,
    and so on. Then, we claim that the list
    $$u_1, \cdots u_k, v_1, \cdots v_l, \cdots$$
    is linearly independent.
    Suppose that
    $$a_1 u_1 + \cdots + a_k u_k + b_1 v_1 + \cdots + b_l v_l = 0$$
    Then, we have
    $$a_1 u_1 + \cdots + a_k u_k = -b_1 v_1 - \cdots - b_l v_l$$
    Since $u_1, \cdots u_k$ are linearly independent,
    we must have $a_1 = \cdots = a_k = 0$.
    Similarly, since $v_1, \cdots v_l$ are linearly independent,
    we must have $b_1 = \cdots = b_l = 0$.
    Therefore, the list is linearly independent.
\end{proof}
\begin{theorem}
    If $T$ has $\dim \V$ distinct eigenvalues,
    then $T$ is diagonalizable.
\end{theorem}
\begin{proof}
    Let $n = \dim \V$. and let $\lambda_1, \cdots \lambda_n$ be the distinct eigenvalues of $T$.
    Let $v_k$ be the eigenvector corresponding to 
    $\lambda_k$ for $k = 1, \cdots n$.
    Then, the list $v_1, \cdots v_n$ is linearly independent.
    Since the list has length $n$, it is a basis of $\V$.
    Therefore, $T$ is diagonalizable.
\end{proof}
\begin{example}
    Let $T \in \mathcal{L}(\mathbb R^3)$ be defined by
    $$T\pmqty{x, y, z} = \pmqty{2x + y, 5y + 3z, 8z}$$
    Then, the matrix of $T$ with respect to the standard basis is
    $$\pmqty{2 & 1 & 0 \\ 0 & 5 & 3 \\ 0 & 0 & 8}$$
    The eigenvalues of $T$ are 2, 5, and 8.
    The corresponding eigenspaces are
    $$E(2, T) = \text{span} \pmqty{1 \\  0 \\  0}, \quad E(5, T) = \text{span} \pmqty{1 \\ 3 \\ 0}, \quad E(8, T) = \text{span} \pmqty{1 \\ 6 \\ 0}$$
    Since the eigenvalues are distinct, $T$ is diagonalizable.
    A basis of $\mathbb R^3$ consisting of eigenvectors of $T$
    is given by
    $$\pmqty{1 \\ 0 \\ 0}, \quad \pmqty{1 \\ 3 \\ 0}, \quad \pmqty{1 \\ 6 \\ 0}$$
    With respect to this basis, the matrix of $T$ is
    $$\pmqty{2 & 0 & 0 \\ 0 & 5 & 0 \\ 0 & 0 & 8}$$
\end{example}
Now, we will find the necessary and sufficien condition
for diagonalizability in terms of the minimal polynomial.
\begin{theorem}
    Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$. 
    $T$ is diagonalizable if and only if the minimal polynomial 
    of $T$ has the form 
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T$.
\end{theorem}
\begin{proof}
    First, suppose that $T$ is diagonalizable.
    Let $v_1, \cdots v_n$ be a basis of $\V$ consisting of eigenvectors of $T$,
    Let $\lambda_1, \cdots \lambda_m$ be the distinct eigenvalues of $T$.
    Then, for each $\lambda_k$ there exists a $v_j$
    such that $Tv_j = \lambda_k v_j$.
    It follows that 
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_m I)v_j = 0$$
    for all $j = 1, \cdots n$.
    Then, the minimal polynomial of $T$ is 
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    Since the minimal polynomial divides this from above, 
    but each eigenvalues is a root of the minimal polynomial,
    it can't have smaller degree. Therefore, 
    the minimal polynomial has the desired form. \\\\
    Conversely,
    suppose the minimal polynomial of $T$ has the form
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T$.
    \\\\ Base Case: $m=1$ \\
    In this case, we have $p(z) = z - \lambda_1$. Thus, $T = \lambda_1 I$.
    Therefore, the matrix of $T$ with respect to any basis is diagonal. \\\\
    Inductive Step: \\
    Let $m > 1$ and suppose that the result holds 
    for all $k < m$.
    Let $U = \text{range }(T - \lambda_m I)$.
    Then, $U$ is invariant under $T$.
    Let $T_U$ be the restriction of $T$ to $U$.
    Then, the minimal polynomial of $T_U$ divides $p(z)/(z - \lambda_m)$.
    Then, the minimal polynomial of $T_U$ has the form
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_{m-1})$$
    By the inductive hypothesis, $T_U$ is diagonalizable and so $U$ has a basis 
    $u_1, \cdots u_m$ consistings of eigenvectors of $T_U$.
    Now, let $u \in \range(T - \lambda_m I) \cap \text{null }(T - \lambda_m I)$ be arbitrary.
    Then, we have
    $Tu = \lambda_m u$ and so 
    $$(T - \lambda_1 I)\cdots (T - \lambda_{m-1})u = 
    (\lambda_m - \lambda_1) \cdots (\lambda_m - \lambda_{m-1})u = 0 $$
    Since the eigenvalues are distinct, we must have $u = 0$.
    Therefore, $\range(T - \lambda_m I) \cap \text{null }(T - \lambda_m I) = \{0\}$.
    and $U + \text{null }(T - \lambda_m I)$ is a direct sum.
    Let $w_1, \cdots w_n$ be a basis of $\text{null }(T - \lambda_m I)$.
    Then,
    $$v_1, \cdots v_m, w_1, \cdots w_n$$
    is linearly independent. Finally, 
    $$m + n = \dim \range(T - \lambda_m I) + \dim \text{null }(T - \lambda_m I) = \dim \V$$
    Therefore, $\V$ has a basis of eigenvectors of $T$ and 
    so $T$ is diagonalizable.
\end{proof}
\begin{theorem}
    Suppose $T$ is diagonalizable and $U$ is a subspace 
    of $\V$ invariant under $T$.
    Then, the restriction of $T$ to $U$ is diagonalizable.
\end{theorem}
\begin{proof}
    The minimal polynomial of $T_U$ divides the minimal polynomial of $T$.
    Since the minimal polynomial of $T$ has the form
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T$,
    the minimal polynomial of $T_U$ also has this form.
    Therefore, $T_U$ is diagonalizable.
\end{proof}
\section{Inner Product Spaces}
We define the norm of $x = (x_1, x_2, \cdots x_n) \in \mathbb{F}^n$ as
$$\|x\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$$
This is called the Euclidean norm.
We define the dot product 
of $x = (x_1, x_2, \cdots x_n)$ and
$y = (y_1, y_2, \cdots y_n)$ in $\mathbb{F}^n$ as
$$x \cdot y = x_1y_1 + x_2y_2 + \cdots + x_ny_n$$
The dot product satisfies the following properties:
\begin{itemize}
    \item $\| x \|^2 = x \cdot x$.
    \item $x \cdot x \geq 0$ with equality if and only if $x = 0$.
    \item $x \cdot y = y \cdot x$.
    \item $(ax + by) \cdot z = a(x \cdot z) + b(y \cdot z)$ for all $a, b \in \mathbb{F}$.
\end{itemize}
We define the inner product on $\mathbb {C}^n$
as 
$$x \cdot y = x_1 \overline{y_1} + x_2 \overline{y_2} + \cdots + x_n \overline{y_n}$$
\begin{definition}[Inner product]
    Let $\V$ be a vector space over $\mathbb{F}$.
    An inner product on $\V$ is a function that takes each ordered pair 
    of vectors $u, v \in \V$ and produces a scalar in $\mathbb{F}$
    $$\inner{\cdot}{\cdot} : \V \times \V \to \mathbb{F}$$
    such that for all $u, v, w \in \V$ and $a \in \mathbb{F}$, the following properties hold:
    \begin{itemize}
        \item $\inner{u}{v} = \overline{\inner{v}{u}}$
        \item $\inner{au + bw}{v} = a \inner{u}{v} + b \inner{w}{v}$
        \item $\inner{v}{v} \geq 0$ with equality if and only if $v = 0$
    \end{itemize}
\end{definition}

Note: If $\mathbb F = \mathbb R$, then 
$\inner{u}{v} = \inner{v}{u}$, i.e conjugate symmetry 
boils down to symmetry.

\begin{example}
    We can define an inner product on the vector space 
    of continuous real-valued functions on 
    $\bqty{-1, 1}$ by 
    $$\inner{f}{g} = \int_{-1}^{1} f(x)g(x) \, dx$$
\end{example}
\begin{definition}[Inner product space]
    A vector space $\V$ equipped with 
    an inner product $\inner{\cdot}{\cdot}$ 
    is called an inner product space.
\end{definition}
Throughout, we assume that 
$\V$ and $\mathbf{W}$ are inner product spaces. 
\begin{lemma}
    \begin{enumerate}
        \item For each fixed $v \in \V$, the function
        $\V \to \mathbb F, u \mapsto \inner{u}{v}$ is linear.
        \item $\inner{0}{v} = \inner{v}{0} = 0$ for all $v \in \V$.
        \item $\inner{v}{0} = 0$ for all $v \in \V$.
        \item $\inner{u}{v + w} = \inner{u}{v} + \inner{u}{w}$ for all $u, v, w \in \V$.
        \item $\inner{u}{av} = \overline{a} \inner{u}{v}$ for all $u, v \in \V$ and $a \in \mathbb F$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item This follows from the definitions of
        linearity and homogeneity of the inner product.
        \item Since any linear map takes 0 to 0,
        we have $\inner{0}{v} = 0$.
        \item This follows from conjugate symmetry
        and part (2).
        \item This follows from linearity in the
        first argument.
        \item This follows from conjugate symmetry.
    \end{enumerate}
\end{proof}
Each inner product determines the norm of a vector. 
\begin{definition}[Norm]
    For a vector $v \in \V$, the norm of $v$ is defined as
    $$\norm{v} = \sqrt{\inner{v}{v}}$$
\end{definition}
\begin{lemma}
    Let $v \in \V$. 
    \begin{enumerate}
        \item $\norm{v} = 0$ iff $v = 0$.
        \item $\norm{av} = \abs{a} \norm{v}$ for all $a \in \mathbb F$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item This follows from the properties of inner products.
        \item We have
        $$\norm{av} = \sqrt{\inner{av}{av}} = \sqrt{a \overline{a} \inner{v}{v}} = \sqrt{\abs{a}^2 \inner{v}{v}} = \abs{a} \norm{v}$$
    \end{enumerate}
\end{proof}
\begin{definition}
    Two vectors $u, v \in \V$ are said to be orthogonal
    if $\inner{u}{v} = 0$.
\end{definition}
Note that $\inner{u}{v} = 0$ iff $\inner{v}{u} = 0$, so 
the order doesn't matter here. 
For $u, v \in \mathbb R^2$, 
the inner product 
$$\inner{u}{v} = \|u\| \|v\| \cos \theta$$
where $\theta$ is the angle between $u$ and $v$.
This also holds more generally in $\mathbb R^n$.
With the usual Euclidean inner product, we have 
$$\inner{u}{v} = 0 \iff \cos \theta = 0 \iff \theta = \pm\frac{\pi}{2}$$
Thus, orthogonal vectors are perpendicular to each other.

\begin{definition}[Orthonormal list]
    A list of vectors $v_1, \cdots v_n$ in $\V$ is called orthonormal
    if $\norm{v_j} = 1$ for all $j = 1, \cdots n$ and 
    $\inner{v_j}{v_k} = 0$ for all $j \neq k$.
\end{definition}
\begin{theorem}
    Suppose $e_1, \cdots e_n$ is an orthonormal list in $\V$.
    Then, 
    $$\norm{a_1 e_1 + a_2 e_2 + \cdots + a_n e_n}^2 =
    \abs{a_1}^2 + \abs{a_2}^2 + \cdots + \abs{a_n}^2$$
    for all $a_1, a_2, \cdots a_n$ in $\mathbb F$.
\end{theorem}
\begin{proof}
    Since each $e_k$ has norm 1, this is immediate 
    from the Pythagorean theorem applied repeatedly.
\end{proof}
\begin{theorem}
    Every orthonormal list of vectors in $\V$
    is linearly independent.
\end{theorem}
\begin{proof}
    Suppose $a_1, a_2, \cdots a_n$ in $\mathbb F$ such that
    $$a_1 e_1 + a_2 e_2 + \cdots + a_n e_n = 0$$
    Then, we have
    $$0 = \norm{a_1 e_1 + a_2 e_2 + \cdots + a_n e_n}^2 =
    \abs{a_1}^2 + \abs{a_2}^2 + \cdots + \abs{a_n}^2$$
    Therefore, $a_1 = a_2 = \cdots = a_n = 0$.
    Thus, the list is linearly independent.
\end{proof}
\newpage
\begin{theorem}[Bessel's Inequality]
    Suppose $e_1, \cdots e_n$ is an orthonormal list in $\V$.
    Then, for every $v \in \V$, we have
    $$\sum_{j=1}^n \abs{\inner{v}{e_j}}^2 \leq \norm{v}^2$$
\end{theorem}
\begin{proof}
    Let $v \in \V$. We breakup $v$ into a component
    in $\text{span}(e_1, \cdots e_n)$
    and a component orthogonal to this subspace.
    Write 
    $$v = \inner{v}{e_1} e_1 + \inner{v}{e_2} e_2 + \cdots + \inner{v}{e_n} e_n +w$$ 
    Let $u$ be the first part of this sum.
    Then, we have
    $$\inner{w, e_k} = \inner{v, e_k} - \inner{v, e_k}\inner{e_k, e_k} = 0$$
    for all $k = 1, \cdots n$.
    This implies $\inner{u}{w} = 0$.
    Therefore, by the Pythagorean theorem, we have
    $$\norm{v}^2 = \norm{u}^2 + \norm{w}^2 \geq \norm{u}^2$$
\end{proof}
\begin{definition}[Orthonormal basis]
    A basis of $\V$ that is an orthonormal list is called an orthonormal basis.
\end{definition}
\begin{theorem}
    Let $\V$ be a finite-dimensional inner product space.
    Then, every orthonormal list of vectors in $\V$ of length
    equal to $\dim \V$ isan orthonormal basis of $\V$.
\end{theorem}
\begin{proof}
    Every orthonormal list is linearly independent. 
    Since the list has the right number of elements, 
    it's a basis for $\V$.
\end{proof}
\begin{theorem}[Parseval's Identity]
    Let $e_1, \cdots e_n$ be an orthonormal basis of $\V$.
    Then, for every $v, u \in \V$, we have
    \begin{enumerate}
        \item $v = \sum_{j=1}^n \inner{v}{e_j} e_j$
        \item $\norm{v}^2 = \sum_{j=1}^n \abs{\inner{v}{e_j}}^2$
        \item $\inner{u}{v} = \sum_{j=1}^n \inner{u}{e_j} \overline{\inner{v}{e_j}}$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Since $e_1, \cdots e_n$ is a basis of $\V$,
        we can write $v$ as a linear combination of these vectors.
        $$v = a_1 e_1 + a_2 e_2 + \cdots + a_n e_n$$
        for some scalars $a_1, a_2, \cdots a_n$ in $\mathbb F$.
        Since the basis is orthonormal, we have
        $$\inner{v}{e_k} = a_k$$
        for all $k = 1, \cdots n$.
        Therefore, we have
        $$v = \sum_{j=1}^n \inner{v}{e_j} e_j$$
        \item This follows from part (1) and the previous theorem.
        \item Using part (1), we have
            $$\inner{u}{v} = \sum_{j=1}^n \inner{u}{e_j} \overline{\inner{v}{e_j}}$$    
    \end{enumerate}
\end{proof}
\begin{definition}[Gram-Schmidt Procedure]
    Let $\V$ be an inner product space.
    The Gram-Schmidt Procedure is a method for
    converting a linearly independent list of vectors
    $v_1, \cdots v_n$ in $\V$ into an orthonormal list
    $e_1, \cdots e_n$ such that 
    $$\text{span}(v_1, \cdots v_k) = \text{span}(e_1, \cdots e_k)$$
    for each $k = 1, \cdots n$.
    The procedure is as follows:
    \begin{enumerate}
        \item Let $e_1 = \frac{v_1}{\norm{v_1}}$.
        \item For $k = 2, \cdots n$, let
        $$w_k = v_k - \sum_{j=1}^{k-1} \inner{v_k}{e_j} e_j$$
        and let 
        $$e_k = \frac{w_k}{\norm{w_k}}$$
    \end{enumerate}
\end{definition}
\begin{proof}
    Wem prove this by induction on $k$.
    Base Case: $k=1$ \\
    In this case, we have
    $$\text{span}(v_1) = \text{span}(e_1)$$
    Inductive Step: \\
    Suppose that for some $2 \leq k < n$, we have
    and $e_1, \cdots e_{k-1}$ is an orthonormal list
    such that $\text{span}(v_1, \cdots v_{k-1}) = \text{span}(e_1, \cdots e_{k-1})$.
    Since $v_1, \cdots v_n$ is linearly independent,
    we have $v_k \notin \text{span}(v_1, \cdots v_{k-1})$.
    Thus, $w_k \neq 0$. Then, we can divide by 
    $\norm{w_k}$ to get $e_k$.
    Now, we have
    $$\text{span}(v_1, \cdots v_k) = \text{span}(e_1, \cdots e_k)$$
    Let $j \in \{1, \cdots k-1\}$.
    To check that 
    $e_k$ is orthogonal to $e_j$
    it suffices to check that $w_k$ is orthogonal to $w_j$.
    We have
    $$\inner{w_k}{w_j} = \inner{v_k - \sum_{i=1}^{k-1} \inner{v_k}{w_i} w_i}{w_j} =
    \inner{v_k}{w_j} - \sum_{i=1}^{k-1} \inner{v_k}{w_i} \inner{w_i}{w_j} = 0$$
    Therefore, $e_1, \cdots e_k$ is an orthonormal list.
\end{proof}
\begin{theorem}
    Every finite dimensional inner product space $\V$ has an orthonormal basis.
\end{theorem}
\begin{proof}
    Let $v_1, \cdots v_n$ be a basis of $\V$.
    Since the basis is linearly independent,
    we can apply the Gram-Schmidt procedure
    to get an orthonormal basis $e_1, \cdots e_n$ of $\V$.
\end{proof}

\begin{theorem}
    Let $\V$ be a finite dimensional inner product space. 
    Then, every orthonormal list of vectors in $\V$
    can be extended to an orthonormal basis of $\V$.
\end{theorem}
\begin{proof}
    Every list of independent vectors can be extended to a basis.
    Then, the basis can be converted to an orthonormal basis
    using the Gram-Schmidt procedure.
\end{proof}
\newpage
\begin{theorem}
    Let $\V$ be a finite dimensional inner product space 
    and $T \in \mathcal{L}(\V)$.
    Then, $T$ has an upper-triangular matrix with respect to some orthonormal 
    basis if and only if the minimal polynomial of $T$ has the form
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are in $\mathbb F$.
\end{theorem}
\begin{proof}
    One direction follows from earlier results.
    Conversely, suppose the minimal polynomial of $T$ has the form
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are in $\mathbb F$.
    Then, T is upper triangular with respect to some basis
    $v_1, \cdots v_n$ of $\V$, and so we have
    $$Tv_k \in \text{span}(v_1, \cdots v_k)$$
    for all $k = 1, \cdots n$. \\ 
    We will apply the Gram-Schmidt procedure
    to convert this basis into an orthonormal basis
    $e_1, \cdots e_n$ of $\V$.  
    Now, the span is unchanged, so we have 
    $$Te_k \in \text{span}(e_1, \cdots e_k)$$
    for all $k = 1, \cdots n$.
\end{proof}

\begin{theorem}[Schur's theorem]
    Let $\V$ be a finite dimensional complex vector space 
    and $T \in \mathcal{L}(\V)$.
    Then, $T$ has an upper-triangular matrix with respect to some orthonormal basis of $\V$.
\end{theorem}
\begin{proof}
    This follows from the previous result and the
    fundamental theorem of algebra.
\end{proof}

\begin{definition}[Linear Functional]
    A linear functional on $\V$ 
    is a linear map from $\V$ to $\mathbb{F}$. 
\end{definition}

\begin{definition}[Dual Space]
    The dual space of $\V$, denoted by $\V^*$,
    is the set of all linear functionals on $\V$.
\end{definition}

If we take $v \in \V$ to be fixed, then the map 
$$u \mapsto \inner{u}{v}$$
is a linear functional on $\V$.

\begin{theorem}[Riezs Representation Theorem]
    Let $\V$ be a finite dimensional inner product space
    and let $\varphi$ be a linear functional on $\V$.
    Then, there exists a unique vector $v \in \V$
    such that 
    $$\varphi(u) = \inner{u}{v}$$
    for all $u \in \V$.
\end{theorem}
\newpage
\begin{proof}
    Existence: Let $e_1, \cdots e_n$ be an orthonormal basis of $\V$.
    Then, 
    $\varphi(u) = \varphi(a_1 e_1 + \cdots a_n e_n)$ for some
    scalars $a_1, \cdots a_n$ in $\mathbb F$.
    Thus, we have
    $$\varphi(u) = a_1 \varphi(e_1) + a_2 \varphi(e_2) + \cdots + a_n \varphi(e_n)$$
    $$\varphi(u) = \inner{u}{e_1} \varphi(e_1) + \cdots \inner{u}{e_n} \varphi(e_n)$$
    $$\varphi(u) = \inner{u}{\overline{\varphi(e_1)} e_1 + \cdots + \overline{\varphi(e_n)} e_n}$$
    Let
    $$v = \overline{\varphi(e_1)} e_1 + \cdots + \overline{\varphi(e_n)} e_n$$
    Then, we have $\varphi(u) = \inner{u}{v}$ for all $u \in \V$. \\\\
    Uniqueness: Suppose there exists $v, w \in \V$ such that
    $$\varphi(u) = \inner{u}{v} = \inner{u}{w}$$
    for all $u \in \V$.
    Then, we have
    $$\inner{u}{v - w} = 0$$
    for all $u \in \V$.
    In particular, taking $u = v - w$, we have
    $$\inner{v - w}{v - w} = 0$$
    Thus, $v - w = 0$ and so $v = w$.
\end{proof}
\begin{definition}[Orthogonal Complement]
    Let $U$ be a subspace of $\V$.
    The orthogonal complement of $U$, denoted by $U^\perp$,
    is the set of all vectors in $\V$ that are orthogonal to every vector in $U$.
    $$U^\perp = \{ v \in \V : \inner{u}{v} = 0 \text{ for all } u \in U \}$$
\end{definition}

\begin{lemma}
    \begin{enumerate}
        \item If $U$ is a subset of $\V$, then $U^\perp$ is a subspace of $\V$.
        \item $\{0\}^\perp = V$
        \item $\V^\perp = \{0\}$
        \item If $U \subseteq \V$, then $U \cap U^\perp = \{0\}$,
        or it is empty.
        \item If $G$ and $H$ are subsets of $\V$
        such that $G \subseteq H$, then $H^\perp \subseteq G^\perp$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item Let $v, w \in U^\perp$ and $a, b \in \mathbb F$.
        Then, for all $u \in U$, we have
        $$\inner{u}{av + bw} = \overline{a} \inner{u}{v} + \overline{b} \inner{u}{w} = 0$$
        Thus, $av + bw \in U^\perp$.
        \item For all $v \in \V$, we have
        $$\inner{0}{v} = 0$$
        Thus, $\V = \{0\}^\perp$.
        \item Let $v \in \V^\perp$. Then, we have
        $$\inner{v}{v} = 0$$
        Thus, $v = 0$ and so $\V^\perp = \{0\}$.
        \item Let $v \in U \cap U^\perp$. Then, we have
        $$\inner{v}{v} = 0$$
        Thus, $v = 0$ and so $U \cap U^\perp = \{0\}$.
        \item Let $v \in H^\perp$. Then, for all $u \in G$,
        since $G \subseteq H$, we have
        $$\inner{u}{v} = 0$$
        Thus, $v \in G^\perp$ and so $H^\perp \subseteq G^\perp$.
    \end{enumerate}
\end{proof}
\begin{theorem}
    Let $U$ be a finite dimensional subspace of $\V$.
    Then,
    $$\dim U \oplus \dim U^\perp = \dim \V$$
    and if $\V$ is finite dimensional, then
    $$\dim U^\perp = \dim \V - \dim U$$
\end{theorem}
\begin{proof}
    First show $\V = U + U^\perp$.
    Let $v \in \V$ be arbitrary.
    Let $e_1, \cdots e_k$ be an orthonormal basis of $U$.
    Then, we can write
    $$v = \inner{v}{e_1} e_1 + \cdots + \inner{v}{e_k} e_k + w$$
    Let $u$ be the first part of this sum.
    Then, we have
    $$\inner{w}{e_j} = \inner{v}{e_j} - \inner{v}{e_j}\inner{e_j}{e_j} = 0$$
    for all $j = 1, \cdots k$.
    This implies $\inner{u}{w} = 0$ and so $w \in U^\perp$.
    Therefore, we have $\V = U + U^\perp$.
    Since $U \cap U^\perp = \{0\}$, this sum is direct.
    Thus, we have
    $$\dim \V = \dim U + \dim U^\perp$$
\end{proof}
\begin{theorem}
    Let $U$ be a finite dimensional subspace 
    of $\V$. Then, $(U^\perp)^\perp = U$.
\end{theorem}
\begin{proof}
    Let $u \in U$. 
    Then, for all $w \in U^\perp$, we have
    $$\inner{u}{w} = 0$$
    But this means that $u \in (U^\perp)^\perp$.
    Thus, we have $U \subseteq (U^\perp)^\perp$.
    Now, suppose $v \in (U^\perp)^\perp$.
    We can write $v$ as
    $$v = u + w$$
    for some $u \in U$ and $w \in U^\perp$.
    Then, for all $z \in U^\perp$, we have  
    $$0 = \inner{v}{z} = \inner{u + w}{z} = \inner{u}{z} + \inner{w}{z} = \inner{w}{z}$$
    In particular, taking $z = w$, we have
    $$0 = \inner{w}{w}$$
    Thus, $w = 0$ and so $v = u \in U$
\end{proof}
\begin{definition}[Orthogonal Projection]
    Let $U$ be a finite-dimensional subspace of $\V$.
    Then, the orthogonal projection
    of $\V$ onto $U$ is the map
    $$P_U : \V \to U$$
    defined as follows: for each $v \in \V$,
    write $v$ as
    $$v = u + w$$
    for some $u \in U$ and $w \in U^\perp$.
    Then, define
    $$P_U v = u$$
\end{definition}
\begin{theorem}
    Suppose $U$ is a finite 
    dimensional subspace of $\V$.
    Then, 
    \begin{enumerate}
        \item $P_U \in \mathcal L(V)$
        \item $P_U u = u$ for all $u \in U$
        \item $P_u w = 0$ for all $w \in U^\perp$
        \item $\text{range }P_U = U$
        \item $\text{null }P_U = U^\perp$
        \item $v - P_U v \in U^\perp$ for all $v \in \V$
        \item $\norm{P_U v} \leq \norm{v}$ for all $v \in \V$
        \item $P^2_U = P_U$
        \item If $e_1, \cdots e_n$ is an orthonormal basis of $U$,
        then for each $v \in \V$, we have
        $$P_U v = \sum_{j=1}^n \inner{v}{e_j} e_j$$
    \end{enumerate}
\end{theorem}
\begin{proof}
    Trivial.
\end{proof}
\begin{theorem}
    Suppose $U$ is a finite dimensional subspace of $\V$
    and $v \in \V$.
    Then, 
    $$\norm{v - P_U v} \leq \norm{v - u}$$
    for all $u \in U$. The inequality holds with equality
    if and only if $u = P_U v$.
\end{theorem}
\begin{proof}
    We have 
    $$\norm{v - P_U v}^2 \leq \norm{v - P_U v}^2 + \norm{P_U v - u}^2$$
    $$\norm{v - P_U}^2 \leq \norm{(v - P_U v) + (P_U v - u)}^2$$
    $$\norm{v - P_U v}^2 \leq \norm{v - u}^2$$
    Now, take square roots,
    $$\norm{v - P_U v} \leq \norm{v - u}$$
    The equality occurs if and only if
    $\norm{P_U v - u} = 0$,
    i.e. $u = P_U v$.
\end{proof}
\section{Operators on Inner Product Spaces}
Throughout, we assume that $\V$ and $\mathbf{W}$ are nonzero, finite-dimensional 
inner product spaces.
\begin{definition}
    Let $T \in \mathcal{L}(\V, \mathbf{W})$.
    The adjoint of $T$ is the operator
    $T^* \in \mathcal{L}(\mathbf{W}, \V)$
    such that 
    $$\inner{Tv}{w} = \inner{v}{T^* w}$$
    for all $v \in \V$ and $w \in \mathbf{W}$.
\end{definition}
This is well-defined. Fix $w \in \mathbf{W}$.
Then, $v \mapsto \inner{Tv}{w}$ is a linear functional on $\V$.
By Riesz representation theorem, there exists a unique
$v_w \in \V$ such that
$$\inner{Tv}{w} = \inner{v}{v_w}$$
then we call $T^* w = v_w$. 
\begin{theorem}
    \begin{enumerate}
        \item $(S + T)^* = S^* + T^* \forall S, T \in \mathcal{L, W}$
        \item $(aT)^* = \overline{a} T^*$ for all $a \in \mathbb F$
        \item $(T^*)^* = T$
        \item $(ST)^* = T^* S^*$
        \item $I^* = I$
        \item If $\V = \mathbf{W}$, then $T$ is invertible
        if and only if $T^*$ is invertible. In this case,
        $$(T^*)^{-1} = (T^{-1})^*$$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item If $S \in \mathcal L(V, W)$ then 
        $$\inner{(S + T)v}{w} = \inner{Sv}{w} + \inner{Tv}{w} = \inner{v}{S^* w} + \inner{v}{T^* w} = \inner{v}{(S^* + T^*) w}$$
        \item We have
        $$\inner{(aT)v}{w} = a \inner{Tv}{w} = a \inner{v}{T^* w} = \inner{v}{\overline{a} T^* w}$$
        \item We have 
        $$\inner{T^* w}{v} = \overline{\inner{v}{T^* w}} = \overline{\inner{Tv}{w}} = \inner{w}{Tv}$$
        \item We have
        $$\inner{(ST)v}{w} = \inner{T v}{S^* w} = \inner{v}{T^* S^* w}$$
        \item We have
        $$\inner{Iv}{w} = \inner{v}{Iw}$$
        \item Suppose $T$ is invertible.
        Then, $TT^{-1} = I$.
        Taking the adjoint of both sides, we have
        $$(T^{-1})^* T^* = I$$
    \end{enumerate}
\end{proof}
\begin{theorem}
    Let $T \in \mathcal{L}(\V, \mathbf{W})$.
    Then, 
    \begin{enumerate}
        \item $\text{null }T^* = (\text{range }T)^\perp$
        \item $\text{range }T^* = (\text{null }T)^\perp$
        \item $\text{null T} = (\text{range }T^*)^\perp$
        \item $\text{range }T = (\text{null }T^*)^\perp$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Let $w \in \mathbf{W}$.
        Then, we have
        $$w \in \text{null }T^* \iff T^* w = 0 \iff \inner{v}{T^* w}$$
        $$ = 0 \text{ for all } v \in \V \iff \inner{Tv}{w} = 0 \text{ for all } v \in \V \iff w \in (\text{range }T)^\perp$$
    \end{enumerate}
    Everything else follows from this.
\end{proof}
\begin{theorem}
    Let $T \in \mathcal L (\V , \mathbf W)$.
    Suppose that $e_1, \cdots e_n$ is an orthonormal basis of $\V$  
    and $f_1, \cdots f_m$ is an orthonormal basis of $\mathbf W$.
    Then, the matrix representation of $T^*$ with respect to
    $f_1, \cdots f_m$ and $e_1, \cdots e_n$
    is the conjugate transpose of the matrix representation of $T$.
    In other words,
    $$(T^*)_{jk} = \overline{T_{kj}}$$
\end{theorem}
\begin{proof}
    Writing $Te_k$ is a linear combination of 
    $f_1, \cdots f_m$, and the coefficients become the $k$-th column
    of the matrix representation of $T$. Since $f_1, \cdots f_m$ is an orthonormal basis,
    we have
    $$Te_k = \inner{Te_k}{f_1} f_1 + \cdots \inner{Te_k}{f_m}f_m$$
    Therefore, 
    $$T_{jk} = \inner{Te_k}{f_j}$$
    Similarly, writing $T^* f_j$ as a linear combination of
    $e_1, \cdots e_n$, we have
    $$T^* f_j = \inner{T^* f_j}{e_1} e_1 + \cdots + \inner{T^* f_j}{e_n} e_n$$
    Thus,
    $$ (T^*)_{kj} = \inner{T^* f_j}{e_k}$$
    Now, by the definition of adjoint, we have
    $$\inner{Te_k}{f_j} = \inner{e_k}{T^* f_j} = \overline{\inner{T^* f_j}{e_k}}$$
    Therefore, we have
    $$(T^*)_{kj} = \overline{T_{jk}}$$
\end{proof}
\begin{definition}
    An operator $T \in \mathcal{L}(\V)$ is called 
    self-adjoint if $T = T^*$. The matrix representation
    of a self-adjoint operator with respect to
    an orthonormal basis is called a Hermitian matrix.
\end{definition}
\begin{theorem}
    Every eigenvalue of a self-adjoint operator is real.
\end{theorem}
\begin{proof}
    Let $\lambda$ be an eigenvalue of a self-adjoint operator $T$
    and let $v$ be a corresponding eigenvector.
    Then, we have
    $$\inner{Tv}{v} = \inner{\lambda v}{v} = \lambda \inner{v}{v}$$
    Since $T$ is self-adjoint, we also have
    $$\inner{Tv}{v} = \inner{v}{Tv} = \inner{v}{\lambda v} = \overline{\lambda} \inner{v}{v}$$
    Therefore, we have
    $$\lambda \inner{v}{v} = \overline{\lambda} \inner{v}{v}$$
    Since $\inner{v}{v} > 0$, we have $\lambda = \overline{\lambda}$. 
\end{proof}
\begin{theorem}
    Let $\V$ be a complex inner product space.
    Then, 
    $$\inner{T v}{v} = 0 \text{ for every $\V \leftrightarrow T = 0$}$$
    This is false for real inner product spaces.
\end{theorem}
\begin{proof}
    For $u, w \in \V$, we have
    $$\inner{T(u + w)}{u + w} = \inner{Tu}{u} + \inner{Tw}{u} + \inner{Tu}{w} + \inner{Tw}{w} = 0$$
    $$\inner{T(u + iw)}{u + iw} = \inner{Tu}{u} + i\inner{Tw}{u} - i\inner{Tu}{w} + \inner{Tw}{w} = 0$$
    Adding these two equations, we have
    $$2 \inner{Tu}{u} + 2 \inner{Tw}{w} = 0$$
    Thus, we have
    $$\inner{Tu}{u} + \inner{Tw}{w} = 0$$
    Substituting this back into the first equation, we have
    $$\inner{Tw}{u} + \inner{Tu}{w} = 0$$
    Substituting this back into the second equation, we have
    $$i\inner{Tw}{u} - i\inner{Tu}{w} = 0$$
    Adding these two equations, we have
    $$2 i \inner{Tw}{u} = 0$$
    Thus, we have $\inner{Tw}{u} = 0$ for all $u, w \in \V$.
    Therefore, $T = 0$.
\end{proof}
\begin{theorem}
    Let $\V$ be a complex inner product space and let $T \in \mathcal{L}(\V)$.
    Then, $T$ is self-adjoint if and only if
    $$\inner{T v}{v} \in \mathbb R \text{ for every } v \in \V$$
\end{theorem}
\begin{proof}
    If $T$ is self-adjoint, then for every $v \in \V$, we have
    $$\overline{\inner{T v}{v}} = \inner{v}{T v} = \inner{T v}{v}$$
    Thus, $\inner{T v}{v} \in \mathbb R$.
    Conversely, suppose $\inner{T v}{v} \in \mathbb R$ for every $v \in \V$.
    Then, we have
    $$\inner{T v}{v} = \overline{\inner{T v}{v}} = \inner{v}{T v}$$
    for every $v \in \V$.
    Therefore, we have
    $$\inner{(T - T^*) v}{v} = 0$$
    for every $v \in \V$.
    By the previous theorem, this implies that $T - T^* = 0$,
    or $T = T^*$.
\end{proof}
\begin{definition}
    An operator $T \in \mathcal{L}(\V)$ is called normal
    if $TT^* = T^* T$, ie it commutes with its adjoint.
\end{definition}
\end{document}