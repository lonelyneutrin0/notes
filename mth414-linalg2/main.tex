\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}                       % custom enum labels
\usepackage{parskip}          
\usepackage{physics}
\usepackage{geometry} 
\usepackage{esint}
\geometry{
  paperwidth=18cm,
  left=8mm,
  right=8mm,
  top=8mm,
  bottom=8mm,
}
\usepackage[framemethod=TikZ]{mdframed}     % graphics and framed envs

\renewcommand{\familydefault}{\sfdefault}      % sans serifs text
\setlength{\parindent}{0pt}                    % no paragraph indentation

% region TITLE CONSTRUCTION
\newlength\mywidth
\mywidth=\wd0
\renewcommand{\contentsname}{\hangindent=\mywidth \courseid: \coursetitle \\ \medskip \LARGE{[MY NAME HERE], \semester}}
% endregion

% region FRAMED ENVIRONMENTS
\newcounter{chapter}\setcounter{chapter}{1}
\newcounter{theo}[chapter]\setcounter{theo}{0}
\newcommand{\numTheo}{\arabic{chapter}.\arabic{theo}}
\newcommand{\mdftheo}[3]{
    \mdfsetup{
        frametitle={
            \tikz[baseline=(current bounding box.east),outer sep=0pt]
            \node[anchor=east,rectangle,fill=#3]
            {\ifstrempty{#2}{\strut #1~\numTheo}{\strut #1~\numTheo:~#2}};
        },
        innertopmargin=4pt,linecolor=#3,linewidth=2pt,
        frametitleaboveskip=\dimexpr-\ht\strutbox\relax,
        skipabove=11pt,skipbelow=0pt
    }
}
\newcommand{\mdfnontheo}[3]{
    \mdfsetup{
        frametitle={
            \tikz[baseline=(current bounding box.east),outer sep=0pt]
            \node[anchor=east,rectangle,fill=#3]
            {\ifstrempty{#2}{\strut #1}{\strut #1:~#2}};
        },
        innertopmargin=4pt,linecolor=#3,linewidth=2pt,
        frametitleaboveskip=\dimexpr-\ht\strutbox\relax,
        skipabove=11pt,skipbelow=0pt
    }
}
\newcommand{\mdfproof}[1]{
    \mdfsetup{
        skipabove=11pt,skipbelow=0pt,
        innertopmargin=4pt,innerbottommargin=4pt,
        topline=false,rightline=false,
        linecolor=#1,linewidth=2pt
    }
}
\newenvironment{theorem}[1][]{
    \refstepcounter{theo}
    \mdftheo{Theorem}{#1}{red!25}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{lemma}[1][]{
    \refstepcounter{theo}
    \mdftheo{Lemma}{#1}{red!15}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{corollary}[1][]{
    \refstepcounter{theo}
    \mdftheo{Corollary}{#1}{red!15}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{definition}[1][]{
    \mdfnontheo{Definition}{#1}{blue!20}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{example}[1][]{
    \mdfnontheo{Example}{#1}{yellow!40}
    \begin{mdframed}[]\relax
}{\end{mdframed}}

\newenvironment{proof}[1][]{
    \mdfproof{black!15}
    \begin{mdframed}[]\relax
\textit{Proof. }}{\end{mdframed}}
% endregion

% region NEW COMMANDS
\newcommand{\ds}{\displaystyle}
\newcommand{\pfn}[1]{\textrm{#1}}  % enables new functions
\newcommand{\mbf}[1]{\mathbf{#1}}  % mathbf
\newcommand{\C}{\mathbb{C}}        % fancy C
\newcommand{\R}{\mathbb{R}}        % fancy R
\newcommand{\Q}{\mathbb{Q}}        % fancy Q
\newcommand{\Z}{\mathbb{Z}}        % fancy Z
\newcommand{\N}{\mathbb{N}}   
\newcommand{\K}{\mathbb{K}}  % fancy N
\newcommand{\V}{\mathbf{V}} %vector space 
\newcommand{\0}{\mathbf{0}} %zero vector 
\newcommand{\from}{\leftarrow}
\renewcommand{\i}[1]{\textit{#1}}
\renewcommand{\b}[1]{\textbf{#1}}
\newcommand{\qed}{$\square$}
\newcommand{\range}{\text{range }}

% endregion

\title{Linear Algebra II}
\author{Hrishikesh Belagali}
\date{August 2025}
\begin{document}
\maketitle
\section{Basics}
\begin{example}
    For a set $\mathcal S$, let $\mathbb F^{\mathcal S}$ be 
    the set of all functions from $\mathcal S$ to $\mathbb F$.
    Then, defined over canonical addition and scalar
    multiplications, $\mathbb F^{\mathcal S}$ is a vector
    space. The additive identity is the zero function $0$,
    defined as $0(x) = 0$. The additive inverse can be defined
    as $-f : \mathcal S \to \mathbb F$ defined as $-f(x) = 
    -(f(x)) \forall x \in \mathcal S$. \\
    
    Note that $\mathbb F^n$ and $\mathbb F^{\infty}$ are 
    special cases of $\mathbb F^{\mathcal S}$, 
    where $\mathcal S$ is a finite set of size $n$ or 
    an infinite set, respectively.
\end{example}

Note that the empty set $\phi$ is not a vector space, nor is it a subspace of any vector space.
\begin{example}
The set of differentiable real-valued functions is a subspace of $\mathbb R^{\mathbb R}$. Note that in calculus, the sum of two continuous functions 
is continuous, and the sum of two differentiable functions is differentiable. Also, scalar multiples of continuous and differentiable functions are continuous and differentiable, respectively.
\end{example}

\begin{definition}
    Let $V_1, \cdots V_n$ be subspaces of a vector space $\V$. Then, the sum of these subspaces is defined as 
    $$V_1 + V_2 + \cdots + V_n = \{ v_1 + v_2 + \cdots + v_n \mid v_i \in V_i \text{ for all } i \}$$
\end{definition}

\begin{example}
    Let $$V_1 = \left \{ (w, w, x, x) \in \mathbb F^4 | w, x \in \mathbb F \right \}$$
    $$V_2 = \left \{ (y, y, y, z) \in \mathbb F^4 | y, z \in \mathbb F \right \}$$
    Now, let $v_1 \in V_1$ and $v_2 \in V_2$. Then, we can write
    $$v_1 = (w_1, w_1, x_1, x_1)$$
    $$v_2 = (y_2, y_2, y_2, z_2)$$
    for some $w_1, x_1, y_2, z_2 \in \mathbb F$. Then, we have
    $$v_1 + v_2 = (w_1 + y_2, w_1 + y_2, x_1 + y_2, x_1 + z_2) \in V_1 + V_2$$
    Let $W$ be defined as 
    $$W = \left \{ (x, x, y, z) \in \mathbb F^4 | x, y, z, \in \mathbb F \right \}$$
    Then, $v_1 + v_2 \in W$ so $V_1 + V_2 \subseteq W$. \\
    Let $w \in W$. Then, we can write
    $$w = (x_w, x_w, y_w, z_w)$$
    for some $x_w, y_w, z_w \in \mathbb F$. Then, we have
    $$w = (x_w, x_w, y_w, z_w) = (x_w, x_w, y_w, y_w) + (0, 0, 0, z_w - y_w) \in V_1 + V_2$$
    $\therefore W = V_1 + V_2$
\end{example}

\begin{lemma}
    For any subspaces $V_1, \cdots V_n$ of a vector space $\V$, $V_1 + \cdots + V_n$ is a subspace of $\V$. It is also the smallest subspace of $V$
    that contains all elements of the form $v_1 + \cdots + v_n$ where $v_i \in V_i$ for all $i$.
\end{lemma}
\begin{proof}
    From the definition and that $V_1, \cdots V_n$ are subspaces,
    Since the subspaces themselves are closed under addition and scalar multiplication, $V_1 + \cdots + V_n$ is also closed under addition and scalar multiplication. Also, the zero vector $\0$ is in each of the subspaces, so $\0 \in V_1 + \cdots + V_n$. Thus, $V_1 + \cdots + V_n$ is a subspace of $\V$. 
\end{proof}

Note: Generally, the set theoretic union is rarely a subspace, except for trivial cases where one space is a subspace of the other. 
However, intersections of subspaces are generally subspaces.

\begin{definition}[Direct sum]
    Let $V_1, \cdots V_n$ be subspaces of a vector space $\V$. Then, the sum $V_1 + \cdots + V_n$ is called a direct sum if each element of $V_1 + \cdots + V_n$ can be written in one and only one way as $v_1 + \cdots + v_n$ where $v_i \in V_i$ for all $i$. 
    In this case, we say that the sum is a direct sum, denoted by 
    $$W = V_1 \oplus V_2 \oplus \cdots \oplus V_n$$
\end{definition}

\begin{example}
    Let $$U = \left \{ (x, x, y) \in \mathbb F^3 | x, y \in \mathbb F \right \}$$
    Let $$W = \left \{ (x, 0, 0) \in \mathbb F^3 | x \in \mathbb F \right \}$$
    Then, $U$ and $W$ are subspaces of $\mathbb F^3$.
    Any arbitrary vector in $\mathbb F^3$ can be written as
    $$\begin{pmatrix}
        a \\
        b \\
        c
    \end{pmatrix} = \begin{pmatrix}
        b \\
        b \\
        c
    \end{pmatrix} + \begin{pmatrix}
        a-b \\
        0 \\
        0
    \end{pmatrix}$$
    Since this is a unique representation, $U \oplus W = \mathbb F^3$.
\end{example}
\begin{theorem}
    Let $V_1, \cdots V_n$ be subspaces of a vector space $\V$. Then, $V_1  + \cdots V_n = V_1 \oplus \cdots \oplus V_n$ if and only if the only way to write $\0$ as $v_1 + \cdots + v_n$ where $v_i \in V_i$ for all $i$ is to take each $v_i = \0$.
    In other words, if $v_1 + \cdots + v_n = \0$ implies that each $v_i = \0$, then the sum is a direct sum.
\end{theorem}
\begin{proof}
    Suppose that $V_1 + \cdots V_n$ is a direct sum. Then, the additive identity can be written as 
    the sum of additive identities from each subspace. By definition of a direct sum, this is the \textit{only}
    way to write the additive identity as a sum. \\ 
    Suppose that the only way to write zero is as the sum 
    of additive identities from each subspace. Consider an arbitrary vector 
    $v \in V$. Suppose that there are two different ways of writing the sum,
    $$v = u_1 + \cdots u_n ; u_k \in V_k$$ 
    $$v = v_1 + \cdots v_n ; v_k \in V_k$$
    Then, we can subtract these two equations 
    $$0 = (u_1 - v_1) + \cdots + (u_n - v_n) ; (u_k - v_k) \in V_k$$
    Since the only way to write zero is as the sum of additive identities from each subspace, we must have $u_k - v_k = 0$ for all $k$. Thus, $u_k = v_k$ for all $k$, and the representation is unique.
    Therefore, by definition, the sum $V_1 + \cdots V_n$ is a direct sum.
\end{proof}

\begin{theorem}
    Let $U$ and $W$ be subspaces of a vector space $\V$. Then, the sum $U + W$ is a direct sum if and only if $U \cap W = \{0\}$.
\end{theorem}
\begin{proof}
    Suppose that $U + W$ is a direct sum. Let $v \in U \cap W$. Then, $v \in U$, and $-v \in W$
    $$0 = v + (-v)$$
    Since the representation is unique, we must have $v = 0$. Thus, $U \cap W = \{0\}$.

    Conversely, suppose that $U \cap W = \{0\}$. Let $u \in U$ and $w \in W$. Then, we can write
    $$u + w = 0$$
    From the previous result, it suffices to show that $u = w = 0$. This implies that $w$ is the additive inverse
    of $u$, meaning $u, w \in U \cap W = \{0\}$. Therefore, $u = w = 0$ and $U + W$ = $U \oplus W$.
\end{proof}
\begin{lemma}
    Suppose $v_1, \cdots v_m$ is a linearly dependent list in V. Then there exists $j \in \{1, \cdots m\}$ such that
    $$v_j \in \text{span}(v_1, \cdots, v_{j-1})$$
    If this condition holds, then $$\text{span} (v_1, \cdots v_m) = \text{span} (v_1, \cdots, v_{j-1}, v_{j+1}, \cdots v_m)$$
\end{lemma}
\begin{proof}
    Since $v_1, \cdots v_m$ is linearly dependent,
    there exist $a_1, \cdots a_m \in \mathbb F$ not all zero
    such that
    $$a_1v_1 + \cdots + a_mv_m = 0$$
    Let $k$ be the largest index such that $a_k \neq 0$.
    Then, $$v_k = -\frac{a_1}{a_k}v_1 - \cdots - \frac{a_{k-1}}{a_k}v_{k-1}$$
    Therefore, $v_k \in \text{span}(v_1, \cdots, v_{k-1})$. \\
    \\Suppose $v_k \in \text{span}(v_1, \cdots, v_{k-1})$. Then, we can write
    $$v_k = b_1v_1 + \cdots + b_{k-1}v_{k-1}$$
    for some $b_1, \cdots, b_{k-1} \in \mathbb F$.
    Let $u \in \text{span}(v_1, \cdots v_m)$. Then, 
    $$u = c_1v_1 + \cdots c_mv_m \text{ for some } c_1, \cdots c_m \in \mathbb F$$
    Substituting for $v_k$, we get
    $$u = c_1v_1 + \cdots + c_{k-1}v_{k-1} + c_k(b_1v_1 + \cdots + b_{k-1}v_{k-1}) + c_{k+1}v_{k+1} + \cdots + c_mv_m$$
    Therefore, 
    $u \in \text{span}(v_1, \cdots, v_{k-1}, v_{k+1}, \cdots v_m)$.
    Proving the other direction is trivial, you can set the coefficients of higher indices than $k$ to zero.
\end{proof}
\begin{theorem}
    Let $V$ be a finite-dimensional vector space. Suppose that 
    $u_1, \cdots u_m$ is linearly independent in $V$ and $w_1, \cdots w_n$.
    Then, $m \leq n$. In other words, any linearly independent list is smaller
    or the same size as any spanning list.
\end{theorem}

\begin{theorem}
    Suppose $U$ is a subspace of a finite-dimensional vector space $V$. Then there exists
    a subspace $W$ of $V$ such that $V = U \oplus W$.
\end{theorem}
\begin{proof}
    Let $u_1, \cdots u_m$ be a basis of $U$. Since $U$ is a subspace of $V$, we can extend this basis to a basis of $V$, say $u_1, \cdots u_m, w_1, \cdots w_n$. Let $W = \text{span}(w_1, \cdots w_n)$. Then, we have
    $$V = U + W$$
    since any vector in $V$ can be written as a linear combination of the basis vectors. \\
    Now, we need to show that the sum is direct. Suppose that 
    $$u + w = 0 ; u \in U, w \in W$$
    Then, we can write
    $$u = a_1u_1 + \cdots + a_mu_m ; a_i \in \mathbb F$$
    $$w = b_1w_1 + \cdots + b_nw_n ; b_i \in \mathbb F$$
    Therefore, we have
    $$a_1u_1 + \cdots + a_mu_m + b_1w_1 + \cdots + b_nw_n = 0$$
    Since the basis vectors are linearly independent, all coefficients must be zero.
    Thus, $u = w = 0$, and the sum is direct.
\end{proof}
\begin{theorem}
    If $V_1$ and $V_2$ are finite-dimensional subspaces of $V$,
    then 
    $$\text{dim}(V_1 + V_2) = \text{dim}(V_1) + \text{dim}(V_2) - \text{dim}(V_1 \cap V_2)$$
\end{theorem}
\begin{proof}
    Let $u_1, \cdots u_m$ be a basis of $V_1 \cap V_2$. We can extend this basis to a basis of $V_1$, say $u_1, \cdots u_m, v_1, \cdots v_k$. Similarly, we can extend the basis of $V_1 \cap V_2$ to a basis of $V_2$, say $u_1, \cdots u_m, w_1, \cdots w_l$. We claim that the list 
    $$u_1, \cdots u_m, v_1, \cdots v_k, w_1, \cdots w_l$$
    is a basis of $V_1 + V_2$. \\
    Since the dimension of $V_1$ is $m+k$ and the dimension of $V_2$ is $m+l$, the dimension of $V_1 + V_2$ is at most $m+k+l$
    Therefore,
    $$\dim (V_1 + V_2) = m + k + l = (m+k) + (m+l) - m = \dim(V_1) + \dim(V_2) - \dim(V_1 \cap V_2)$$
    It remains to show that the list spans $V_1 + V_2$ and is linearly independent. \\
    Let $v \in V_1 + V_2$. Then, we can write 
    $$v = v_1 + v_2 ; v_1 \in V_1, v_2 \in V_2$$
    Since $u_1, \cdots u_m, v_1, \cdots v_k$ is a basis of $V_1$, we can write
    $$v_1 = a_1u_1 + \cdots + a_mu_m + b_1v_1 + \cdots + b_kv_k ; a_i, b_i \in \mathbb F$$
    Similarly, we can write
    $$v_2 = c_1u_1 + \cdots + c_mu_m + d_1v_1 + \cdots + d_kv_k ; c_i, d_i \in \mathbb F$$  
    Therefore, we have
    $$v = (a_1 + c_1)u_1 + \cdots + (a_m + c_m)u_m + b_1v_1 + \cdots + b_kv_k + d_1w_1 + \cdots + d_lw_l$$
    Thus, the list spans $V_1 + V_2$. \\
    To show that the list is linearly independent, suppose we have a linear combination
    $$\sum_{i=1}^m \alpha_i u_i + \sum_{j=1}^k \beta_j v_j + \sum_{l=1}^l \gamma_l w_l = 0$$
    for some scalars $\alpha_i, \beta_j, \gamma_l \in \mathbb F$. We need to show that all coefficients must be zero. \\
    Since $u_1, \cdots u_m, v_1, \cdots v_k$ is a basis of $V_1$, we can write
    $$v_1 = a_1u_1 + \cdots + a_mu_m + b_1v_1 + \cdots + b_kv_k ; a_i, b_i \in \mathbb F$$
    Similarly, we can write
    $$v_2 = c_1u_1 + \cdots + c_mu_m + d_1v_1 + \cdots + d_kv_k ; c_i, d_i \in \mathbb F$$
    Complete ts later
\end{proof}
\section{Eigenvectors and eigenvalues}
\subsection{Invariant subspaces}
\begin{definition}[Invariant subspace]
    Let $T \in \mathcal L(\V)$. A subspace $U$ of $\V$ is called invariant under $T$ if 
    $$u \in U \implies T(u) \in U$$
    In other words, $U$ is invariant under $T$ if $T(U) \subseteq U$.
\end{definition}
\begin{definition}[Eigenvalues and eigenvectors]
    A number $\lambda \in \mathbb F$ is called an eigenvalue of $T \in \mathcal L(\V)$ if there exists a non-zero vector $v \in \V$ such that
    $$T(v) = \lambda v$$
    Such a vector $v$ is called an eigenvector corresponding to the eigenvalue $\lambda$.
\end{definition}
\begin{definition}[Polynomials of linear operators]
    Let $m$ be a positive integer. Define $T^m$ as $T \circ T \circ \cdots \circ T$ ($m$ times). Define $T^0 = I$. Define $T^{-m} = (T^{-1})^m$ if $T$ is invertible.
    Then, 
    let $p(z) = a_mz^m + a_{m-1}z^{m-1} + \cdots + a_1z + a_0$ be a polynomial with coefficients in $\mathbb F$. Then, we can define the polynomial of the operator $T$ as
    $$p(T) = a_mT^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I$$
\end{definition}
\begin{lemma}
If $p$, $q$ are polynomials and $T \in \mathcal L(\mathbf V)$, then $(pq)(T) = p(T)q(T) = q(T)p(T)$.
\end{lemma}
\begin{proof}
    Let $p(z) = a_mz^m + a_{m-1}z^{m-1} + \cdots + a_1z + a_0$ and $q(z) = b_nz^n + b_{n-1}z^{n-1} + \cdots + b_1z + b_0$. Then, we have
    $$p(T) = a_mT^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I$$
    $$q(T) = b_nT^n + b_{n-1}T^{n-1} + \cdots + b_1T + b_0I$$
    Now, we can compute the product $p(T)q(T)$ as follows:
    \begin{align*}
        p(T)q(T) &= (a_mT^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I)(b_nT^n + b_{n-1}T^{n-1} + \cdots + b_1T + b_0I) \\
        &= a_mb_nT^{m+n} + (a_mb_{n-1} + a_{m-1}b_n)T^{m+n-1} + \cdots + (a_0b_0)I
    \end{align*}
    This is exactly the polynomial $(pq)(T)$, where $(pq)(z) = p(z)q(z)$.
    Since multiplication of polynomials is commutative, we also have $p(T)q(T) = q(T)p(T)$.
    \end{proof}
    \begin{theorem}
    Let $T \in \mathcal L(\V)$ and $p \in \mathcal P(\mathbb F)$. Then, $\text{null } p(T)$ and $\text{range } p(T)$ are invariant under $T$.
    \end{theorem}
    \begin{proof}
        Let $u \in \text{null } p(T)$. Then, we have
        $$p(T)(u) = 0$$
        Applying $T$ to both sides, we get
        $$p(T)(Tu) = (p(T)T)u = (Tp(T))u = T(p(T)u) = T(0) = 0$$
        Thus, $T(u) \in \text{null } p(T)$, showing that $\text{null } p(T)$ is invariant under $T$.

        Now, let $v \in \text{range } p(T)$. Then, we can write
        $$v = p(T)(w)$$
        for some $w \in \V$. Applying $T$, we get
        $$T(v) = T(p(T)(w)) = p(T)(T(w))$$
        Thus, $T(v) \in \text{range } p(T)$, showing that $\text{range } p(T)$ is invariant under $T$.
    \end{proof}
    \subsection{Characteristic polynomials}
    \begin{definition}[Monic polynomial]
        A polynomial $p \in \mathcal P(\mathbb F)$ is called monic if the leading coefficient is 1.
    \end{definition}
    \begin{definition}[Minimal polynomial]
        Let $T \in \mathcal L(\V)$. A monic polynomial $p$ of smallest degree such that $p(T) = 0$ is called the minimal polynomial of $T$.
    \end{definition}
    \begin{theorem}
        Let $\mathbf V$ be finite dimensional. Let $T \in \mathcal L(\mathbf V)$. Then, there is a 
        unique monic polynomial $p$ in $\mathcal P(\mathbb F)$ of smallest degree such that $p(T) = 0$.
        Furthermore, the degree of $\deg p \leq \dim \mathbf V$.
    \end{theorem}
    \begin{proof}\\\\
        \textbf{Existence:} We will prove it by induction on the dimension of $\V$. \\\\
        \textit{Base case:} \\ If $\dim \V = 0$, then $\V = \{0\}$. Thus, $T$ is the zero operator, and we can take $p(z) = 1$. \\\\
        \textit{Inductive step:} \\
        Suppose that $\dim \V > 0$ and suppose that the results is true for all operators
        on all vector spaces of dimension strictly less than $\dim \V$. Let $u \in \V, u \neq 0$
        The list $u, Tu, T^2u, \cdots T^nu$ must be linearly dependent for some $n \leq \dim \V$.
        Then there exists a smallest positive integer $m$ such that
        $T^mu$ is a linear combination of the previous $T^ku$'s. Thus, we can write
        $$T^mu = -a_{m-1}T^{m-1}u - \cdots - a_1Tu - a_0u$$
        for some $a_0, \cdots a_{m-1} \in \mathbb F$. Let
        $$p_1(z) = z^m + a_{m-1}z^{m-1} + \cdots + a_1z + a_0$$
        Then, we have $p_1(T)(u) = 0$. \\
        Since $\text{null } p_1(T)$ is invariant under $T$, $u, Tu, T^2u, \cdots T^{m-1}u \in \text{null } p_1(T)$.
        Note that from the definition of $m$, the list $u, Tu, T^2u, \cdots T^{m-1}u$ is linearly independent.
        Therefore, $\dim \text{null } p_1(T) \geq m$. Then, 
        $$\dim \text{range } p_1(T) = \dim \V - \dim \text{null } p_1(T) \leq \dim \V - m < \dim \V \leq n - m$$
        Since $\text{range } p_1(T)$ is invariant under $T$, we can apply the inductive hypothesis to the operator
        $T|_{\text{range } p_1(T)}$. Thus, there exists a unique monic polynomial $q \in \mathcal P(\mathbb F)$ of degree $\leq n - m$
        such that $q(T|_{\text{range } p_1(T)}) = 0$. Then, for all $v \in \V$, we have 
        $p_1(T) \in \text{ range}q(T)$. Thus, $q(T)(p_1(T)(v)) = 0$. Therefore, we have
        $$(q p_1)(T)(v) = q(T)(p_1(T)(v)) = 0$$
        for all $v \in \V$. Therefore, $(qp_1)(T) = 0$, and we can take $p = qp_1$ as our minimal polynomial.
        
        \textbf{Uniqueness:} Suppose $p$ and $q$ are both monic polynomials of smallest degree
        such that $p(T) = q(T) = 0$. Then $(p-q)(T) = 0$, and since $p$ and $q$ have the
        same degree and leading coefficient, $p-q$ has degree strictly less than $\deg p$. Note that 
        $p-q$ is not necessarily monic. However, we can write $p-q = c r$ where $c$ is the leading coefficient of $p-q$ and $r$ is a monic polynomial. Then, we have
        $$0 = (p-q)(T) = c r(T)$$
        By minimality of $\deg p$, we must have $p-q = 0$, so $p = q$.
    \end{proof}
\begin{definition}
    Let $\V$ be finite-dimensional and $T \in \mathcal L(\V)$. Let $p(z)$ be the 
    minimal polynomial of $T$. Then, 
    \begin{itemize}
        \item The zeros of $p$ are the eigenvalues of $T$.
        \item If $\V$ is a complex vector space, then 
        $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
        where $\lambda_1, \cdots \lambda_m$ are the eigenvalues of $T$ (possibly repeated).
    \end{itemize}
\end{definition}
\begin{proof}
    \textbf{Forward: If $\lambda$ is a zero of $p$, then $\lambda$ is an eigenvalue of $T$.} \\
    Suppose that $p(\lambda) = 0$. Then, we can write
    $$p(z) = (z - \lambda)q(z)$$
    where $q(z)$ is a monic polynomial.
    Then, we have 
    $$p(T) = (T - \lambda I)q(T) = 0$$
    For some $v \in \V$ such that $q(T)v \neq 0$, we have 
    $$(T - \lambda I)(q(T)v) = 0$$
    Since $\deg{q} = \deg{p} - 1 < \deg{p}$, such a $v$ exists.
    Thus, $T(q(T)v) = \lambda (q(T)v)$, and $q(T)v$ is an eigenvector corresponding to the eigenvalue $\lambda$. \\\\
    \textbf{Reverse: If $\lambda$ is an eigenvalue of $T$, then $\lambda$ is a zero of $p$.} \\
    Suppose that $\lambda$ is an eigenvalue of $T$. Then, there exists a non-zero vector $v \in \V$ such that
    $$T(v) = \lambda v$$
    Then, we have
    $$T(Tv) = T(\lambda v) = \lambda T(v) = \lambda^2 v$$
    Continuing this way, we can show that
    $$T^k(v) = \lambda^k v$$
    for all non-negative integers $k$. Then, we have
    $$p(T)(v) = (T^m + a_{m-1}T^{m-1} + \cdots + a_1T + a_0I)(v) = ( \lambda^m + a_{m-1}\lambda^{m-1} + \cdots + a_1\lambda + a_0)v = p(\lambda)v$$
    Since $p(T) = 0$, we have $p(\lambda)v = 0$. Since $v \neq 0$, we must have $p(\lambda) = 0$.

    By the fundamental theorem of algebra, if $\V$ is a complex vector space, then $p(z)$ can be factored into linear factors. Since the zeros of $p$ are the eigenvalues of $T$, we can write
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are the eigenvalues of $T$ (possibly repeated).
\end{proof}
\begin{corollary}
    Let $\V$ be a nonzero, finite-dimensional, complex vector space and $T \in \mathcal L(\V)$.
    Then, $T$ has an eigenvalue.
\end{corollary}
Note: The theorem is not true for infinite-dimensional vector spaces.
For example, consider the vector space $\mathbb C^{\infty}$ and the right shift operator $T$ defined as
$$T(x_1, x_2, x_3, \cdots) = (0, x_1, x_2, x_3, \cdots)$$
Then, $T$ has no eigenvalues. To see this, suppose that $\lambda$ is an eigenvalue of $T$. Then, there exists a non-zero vector $v = (x_1, x_2, x_3, \cdots)$ such that
$$T(v) = \lambda v$$
This gives us the equation
$$(0, x_1, x_2, x_3, \cdots) = \lambda (x_1, x_2, x_3, \cdots)$$
Equating components, we find that
$$0 = \lambda x_1$$
$$x_1 = \lambda x_2$$
$$x_2 = \lambda x_3$$
$$\vdots$$
If $\lambda \neq 0$, this implies that $x_1 = x_2 = x_3 = \cdots = 0$, contradicting the assumption that $v$ is non-zero. Therefore, we must have $\lambda = 0$. However, if $\lambda = 0$, then we have
$$T(v) = (0, x_1, x_2, x_3, \cdots) = 0$$
which again implies that $v = 0$. Thus, we conclude that $T$ has no eigenvalues. \\\\ 
Recall that $q(z)$ divides $p(z)$ if there exists a polynomial $r(z)$ such that $p(z) = q(z)r(z)$.
\begin{theorem}
    Let $\V$ be finite-dimensional and $T \in \mathcal L(\V)$.
    Let $p(z)$ be the minimal polynomial of $T$.
    Let $q(z) \in \mathcal P(\mathbb F)$.  Then, $q(T) = 0$ if and only if $p(z)$ divides $q(z)$.
\end{theorem}
\begin{proof}
    If $q = ps$ for some $s \in \mathcal P(\mathbb F)$, then
    $q(T) = p(T)s(T) = 0$, $s(T) = 0$. \\
    Conversely, suppose that $q(T) = 0$. Using polynomial long division, we can write
    $$q(z) = p(z)s(z) + r(z)$$
    where $\deg r < \deg p$. Then, we have
    $$r(T) = q(T) - p(T)s(T) = 0 - 0 = 0$$
    Since $\deg r < \deg p$, we must have $r(z) = 0$. Thus, we conclude that $p(z)$ divides $q(z)$.
\end{proof}
\begin{corollary}
    Let $\V$ be finite-dimensional and $T \in \mathcal L(\V)$.
    and let $U \subseteq \V$ be invariant under $T$.
    Let $T_U$ be the restriction of $T$ to $U$.
    Thus, the minimal polynomial of $T_U$ divides the minimal polynomial of $T$.
\end{corollary}

\begin{theorem}
    Let $\V$ be finite dimensional and $T \in \mathcal L(\V)$.
    Then, $T$ is not invertible if and only if the minimal polynomial of $T$ does not have a constant term.
\end{theorem}
\begin{proof}
    Let $p$ be the minimal polynomial of $T$.
    Suppose that $T$ is not invertible. Then,
    $\text {null }{T} \neq \{0\}$. Then, 0 is an eigenvalue of T.
    Thus, $p(0) = 0$, so $p$ does not have a constant term.
    Conversely, suppose that $p$ does not have a constant term.
    Then, 0 is an eigenvalue of $T$, so $\text {null }{T} \neq \{0\}$.
    Thus, $T$ is not invertible.
\end{proof}
\begin{theorem}
    Every operator on an odd-dimensional real vector space has an eigenvalue.
\end{theorem}
\begin{proof} 
    later
\end{proof}
\subsection{Upper-triangular matrices}
When discussing operators on $\V$, we'll fix a basis for $\V$.
Then, we can represent the operator as a matrix.
\begin{definition}[Upper triangular matrix]
    Let $T \in \mathcal L(\V)$, where $\V$ is finite-dimensional. The matrix of $T$
    with respect to a basis $v_1, \cdots v_n$ of $\V$ is called upper-triangular if
    $$Tv_j \in \text{span}(v_1, \cdots v_j)$$
    for all $j = 1, \cdots n$. In other words, the matrix of $T$ has all entries below the main diagonal equal to zero. 
\end{definition}
\begin{theorem}
    Let $T \in \mathcal{L}(\V)$, where $\V$ is finite dimensional.
    Let $v_1, \cdots v_n$ be a basis of $\V$.
    Then, the following are equivalent:
    \begin{enumerate}
    \item The matrix of $T$ with respect to $v_1, \cdots v_n$ is upper-triangular. 
    \item $span(v_1, \cdots v_j)$ is invariant under $T$ for $j = 1, \cdots n$.
    \item $Tv_j \in \text{span}(v_1, \cdots v_{j-1})$ for $j = 1, \cdots n$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    $(1) \implies (2)$: \\ 
    Suppose $k \in \{1, \cdots n\}$ and let $j \in \{1, \cdots k\}$. Then, we can write
    $$Tv_j = a_1v_1 + \cdots + a_jv_j$$
    for some $a_1, \cdots a_j \in \mathbb F$.
    Therefore, $Tv_j \in \text{span}(v_1, \cdots v_k)$.
    Since $j$ was arbitrary, we conclude that $\text{span}(v_1, \cdots v_k)$ is invariant under $T$. \\\\
    $(2) \implies (3)$: \\
    From the definition of $\text{span}(v_1, \cdots v_j)$ being invariant under $T$, we have
    $$Tv_k \in \text{span}(v_1, \cdots v_k)$$
    $(3) \implies (1)$: \\
    Suppose $k \in \{1, \cdots n\}$ and let $j \in \{1, \cdots k\}$. Then, we can write
    $$Tv_j = a_1v_1 + \cdots + a_jv_j$$
    for some $a_1, \cdots a_j \in \mathbb F$.
    Therefore, $Tv_j \in \text{span}(v_1, \cdots v_k)$.
    Since $j$ was arbitrary, we conclude that $\text{span}(v_1, \cdots v_k)$ is invariant under $T$. \\
\end{proof}
\begin{theorem}
    Let $T \in \mathcal{L}(\V)$, 
    where $\V$ is finite dimensional.
    If $\mathcal{M}(T)$ is upper-triangular with respect to some basis of $\V$,
    and $\lambda_1, \cdots \lambda_n$ are the entries on the main diagonal of $\mathcal{M}(T)$,
    then 
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_n I) = 0$$
\end{theorem}
\begin{proof}
    Let $p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_n)$.
    It suffices to show that $(T- \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_n I)$ vanishes
    on $\text{span} (v_1, \cdots v_k)$ for all $k= 1, \cdots n$. For $k=1$, 
    this is true since $Tv_1 = \lambda_1 v_1$. \\
    Now note $(T - \lambda_2 I) \in \text{span}(v_1)$, so 
    $(T - \lambda_1 I)(T - \lambda_2 I)v_2 = 0$.
    Continuing this way, we can show that
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_k I)v_k = 0$$
    for all $k = 1, \cdots n$. Thus, we conclude that
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_n I) = 0$$
\end{proof}
\begin{theorem}
    Let $T \in \mathcal L(\V)$, where $\V$ is finite-dimensional.
   Then, the eigenvalues are precisely the entries on the main diagonal of any upper-triangular matrix representing $T$.
\end{theorem}
\begin{proof}
    Let $p(z)$ be the minimal polynomial of $T$.
    Let $q(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_n)$,
    where $\lambda_1, \cdots \lambda_n$ are the entries on the main diagonal of the upper-triangular matrix representing $T$.
    Then, $q(T) = 0$. Thus, $p$ divides $q$. Then,
    $p(z)$ must be of the form 
    $$p(z) = (z - \lambda_{i_1})(z - \lambda_{i_2}) \cdots (z - \lambda_{i_m})$$
    where $i_1, i_2, \cdots i_m$ are some indices in $\{1, 2, \cdots n\}$.
    \\ 
    Let $j \in I = \{1, 2, \cdots n\}$ be arbitrary. Since the $j$th 
    diagonal of a product of upper-triangular matrix is the 
    product of the $j$th diagonals of the matrices, the $j$th
    element of $p(T)$ is $$\prod_{i\in I} (\lambda_j - \lambda_i) = 0$$
    Thus, there exists $i \in I$ such that $\lambda_j - \lambda_i = 0$, or $\lambda_j = \lambda_i$.
    Therefore, $\lambda_j$ is an eigenvalue of $T$.
\end{proof}
\begin{theorem}
    Let $\V$ be a finite-dimensional vector space and $T \in \mathcal{L}(\V)$.
    Then, $T$ has an upper-triangular matrix with respect to 
    some basis of $\V$ if and only if 
    the minimal polynomial of $T$ has the form 
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_n)$$
    where $\lambda_1, \cdots \lambda_n$ are in $\mathbb F$ (not necessarily distinct).
\end{theorem}
Note: This is always true for $\mathbb C$, it becomes an issue for 
$\mathbb R$.
\begin{proof}
    First, suppose that the matrix of $T$ is upper-triangular
    with respect to some basis $\mathcal{B} = \{v_1, \cdots v_n \}$.
    Let $\alpha_1, \cdots \alpha_n$ be the 
    diagonal entries of the matrix of $T$ and
    $q(z) = (z - \alpha_1)(z - \alpha_2) \cdots (z - \alpha_n)$.
    Then, we have $q(T) = 0$. Let $p(z)$ be the minimal polynomial of $T$.
    Then, $p$ divides $q$, so $p(z)$ must be of the form
    $$p(z) = (z - \alpha_1)(z - \alpha_2) \cdots (z - \alpha_n).$$
    Conversely, suppose that the minimal polynomial of $T$ has the form
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are in $\mathbb F$.
    We prove the result by induction on $m = \deg p$. \\\\
    Base case: $m=1$ \\ 
    In this case, we have $p(z) = z - \lambda_1$. Thus, $T = \lambda_1 I$.
    Therefore, the matrix of $T$ with respect to any basis is upper-triangular. \\\\
    Inductive step: \\
    Suppose that the result holds for all $k < m$. Let 
    $U = \text{range }(T - \lambda_m I)$.
    Then, $U$ is invariant under $T$.
    Let $T_U$ be the restriction of $T$ to $U$.
    Then, the minimal polynomial of $T_U$ divides $p(z)/(z - \lambda_m)$.
    Then, the minimal polynomial of $T_U$ has the form
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_{m-1})$$
    By the inductive hypothesis, there exists a basis
    $u_1, \cdots u_m$ of $U$ 
    such that the matrix of $T_U$ 
    is upper triangular with respect to this basis.
    Then, for $k \in \{ 1, \cdots m\}$, we have 
    $$Tu_k = T|_u (u_k) \in \text{span }(u_1, \cdots u_k)$$
    Now, extend this basis of $U$ to a basis of $\V$, say
    $u_1, \cdots u_m, v_1, \cdots v_n$
    where $n = \dim \V - \dim U$.
    If $k \in \{1, \cdots n\}$, then we have
    $$Tv_k = (T - \lambda_m I)v_k + \lambda_m v_k \in \text{span }
    (u_1, \cdots u_m, v_1, \cdots v_k)$$
    Therefore, $T$ is upper-triangular with respect to the basis
    $u_1, \cdots u_m, v_1, \cdots v_n$.
\end{proof}
\begin{corollary}
    Let $\V$ be a finite dimensional complex vector space 
    and $T \in \mathcal{L}(\V)$.
    Then, $T$ has an upper-triangular matrix with respect to some basis of $\V$.
\end{corollary}
\begin{proof}
    It follows from the fundamental theorem of algebra.
\end{proof}

\begin{definition}[Diagonalizable]
Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$.
We say that $T$ is diagonalizable if 
$T$ has a diagonal matrix with respect to some basis of $\V$. 
\end{definition}
\begin{example}
    Define $T \in \mathcal{L}(\mathbb R^2)$ by
    $$T\pmqty{x, y} = \pmqty{41x + 7y, -20x + 74y}$$
    Then, the matrix of $T$ wrt the standard basis is
    $$\mathcal{M}(T) = \pmqty{41 & 7 \\ -20 & 74}$$
    With respect to the basis $v_1 = \pmqty{1 \\ 4}, v_2 = \pmqty{7 \\ 5}$,
    the matrix of $T$ is
    $$\mathcal{M}(T) = \pmqty{69 & 0 \\ 0 & 46}$$
    Therefore, $T$ is diagonalizable.
\end{example}

It is convenient to give a name to the set of all eigenvectors (and $\0$).
\begin{definition}[Eigenspace]
    Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$.
    Let $\lambda$ be an eigenvalue of $T$.
    The eigenspace corresponding to $\lambda$ is defined as
    $$E(\lambda, T) = \text{null }(T - \lambda I)$$
\end{definition}
Note that $T$ restricted to $E(\lambda, T)$ is just multiplication by $\lambda$.
\begin{theorem}
    If $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T \in \mathcal{L}(\V)$,
    then then the sum of the corresponding eigenspaces is a direct sum, and
    $\dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T) \leq \dim \V$.
\end{theorem}
\begin{proof}
    Suppose $v_1, \cdots v_m = 0$, where 
    $v_k \in E(\lambda_k, T)$ for $k = 1, \cdots m$.
    We proved before that eigenvectors 
    corresponding to distinct eigenvalues are linearly
    independent. Thus, $v_1 = \cdots = v_m = 0$.
    Therefore, the sum is direct.
    The dimension inequality follows 
    from the properties of direct sums.
\end{proof}
We will be developing a criterion for diagonalizability.
\begin{theorem}
    Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$.
    Let $\lambda_1, \cdots \lambda_m$ be the distinct eigenvalues of $T$.
    Then, the following are equivalent:
    \begin{enumerate}
        \item $T$ is diagonalizable.
        \item $\V$ has a basis consisting of eigenvectors of $T$.
        \item $\V = E(\lambda_1, T) \oplus E(\lambda_2, T) \oplus \cdots \oplus E(\lambda_m, T)$
        \item $\dim E(\lambda_1, T) + \dim E(\lambda_2, T) + \cdots + \dim E(\lambda_m, T) = \dim \V$
\end{enumerate}
\end{theorem}
\begin{proof}
    $(1) \iff (2)$: \\ 
    Note that $T$ has a diagonal matrix
    with respect to some basis $(v_1, \cdots v_n)$
    if and only if 
    $$Tv_j = \lambda_j v_j$$
    This means that the basis consists
    of eigenvectors of $T$. \\\\
    $(2) \iff (3)$: \\
    Since $\V$ has a basis of eigenvectors of $T$,
    every vector in $\V$ can be written
    as a linear combination of these eigenvectors.
    $v = v_1, \cdots v_n$ where
    $v_k \in E(\lambda_{i_k}, T)$ for some $i_k \in \{1, \cdots m\}$.
    Thus, $$\V = E(\lambda_1, T) + E(\lambda_2, T) + \cdots + E(\lambda_m, T)$$.
    $$\V = E(\lambda_1, T) \oplus E(\lambda_2, T) \oplus \cdots \oplus E(\lambda_m, T)$$ 
    $(3) \iff (4)$: \\
    This follows from the dimension formula for direct sums. \\\\
    $(4) \implies (2)$: \\
    Let $u_1, \cdots u_k$ be a basis of $E(\lambda_1, T)$,
    $v_1, \cdots v_l$ be a basis of $E(\lambda_2, T)$,
    and so on. Then, we claim that the list
    $$u_1, \cdots u_k, v_1, \cdots v_l, \cdots$$
    is linearly independent.
    Suppose that
    $$a_1 u_1 + \cdots + a_k u_k + b_1 v_1 + \cdots + b_l v_l = 0$$
    Then, we have
    $$a_1 u_1 + \cdots + a_k u_k = -b_1 v_1 - \cdots - b_l v_l$$
    Since $u_1, \cdots u_k$ are linearly independent,
    we must have $a_1 = \cdots = a_k = 0$.
    Similarly, since $v_1, \cdots v_l$ are linearly independent,
    we must have $b_1 = \cdots = b_l = 0$.
    Therefore, the list is linearly independent.
\end{proof}
\begin{theorem}
    If $T$ has $\dim \V$ distinct eigenvalues,
    then $T$ is diagonalizable.
\end{theorem}
\begin{proof}
    Let $n = \dim \V$. and let $\lambda_1, \cdots \lambda_n$ be the distinct eigenvalues of $T$.
    Let $v_k$ be the eigenvector corresponding to 
    $\lambda_k$ for $k = 1, \cdots n$.
    Then, the list $v_1, \cdots v_n$ is linearly independent.
    Since the list has length $n$, it is a basis of $\V$.
    Therefore, $T$ is diagonalizable.
\end{proof}
\begin{example}
    Let $T \in \mathcal{L}(\mathbb R^3)$ be defined by
    $$T\pmqty{x, y, z} = \pmqty{2x + y, 5y + 3z, 8z}$$
    Then, the matrix of $T$ with respect to the standard basis is
    $$\pmqty{2 & 1 & 0 \\ 0 & 5 & 3 \\ 0 & 0 & 8}$$
    The eigenvalues of $T$ are 2, 5, and 8.
    The corresponding eigenspaces are
    $$E(2, T) = \text{span} \pmqty{1 \\  0 \\  0}, \quad E(5, T) = \text{span} \pmqty{1 \\ 3 \\ 0}, \quad E(8, T) = \text{span} \pmqty{1 \\ 6 \\ 0}$$
    Since the eigenvalues are distinct, $T$ is diagonalizable.
    A basis of $\mathbb R^3$ consisting of eigenvectors of $T$
    is given by
    $$\pmqty{1 \\ 0 \\ 0}, \quad \pmqty{1 \\ 3 \\ 0}, \quad \pmqty{1 \\ 6 \\ 0}$$
    With respect to this basis, the matrix of $T$ is
    $$\pmqty{2 & 0 & 0 \\ 0 & 5 & 0 \\ 0 & 0 & 8}$$
\end{example}
Now, we will find the necessary and sufficien condition
for diagonalizability in terms of the minimal polynomial.
\begin{theorem}
    Let $\V$ be finite dimensional and $T \in \mathcal{L}(\V)$. 
    $T$ is diagonalizable if and only if the minimal polynomial 
    of $T$ has the form 
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T$.
\end{theorem}
\begin{proof}
    First, suppose that $T$ is diagonalizable.
    Let $v_1, \cdots v_n$ be a basis of $\V$ consisting of eigenvectors of $T$,
    Let $\lambda_1, \cdots \lambda_m$ be the distinct eigenvalues of $T$.
    Then, for each $\lambda_k$ there exists a $v_j$
    such that $Tv_j = \lambda_k v_j$.
    It follows that 
    $$(T - \lambda_1 I)(T - \lambda_2 I) \cdots (T - \lambda_m I)v_j = 0$$
    for all $j = 1, \cdots n$.
    Then, the minimal polynomial of $T$ is 
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    Since the minimal polynomial divides this from above, 
    but each eigenvalues is a root of the minimal polynomial,
    it can't have smaller degree. Therefore, 
    the minimal polynomial has the desired form. \\\\
    Conversely,
    suppose the minimal polynomial of $T$ has the form
    $$p(z) = (z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T$.
    \\\\ Base Case: $m=1$ \\
    In this case, we have $p(z) = z - \lambda_1$. Thus, $T = \lambda_1 I$.
    Therefore, the matrix of $T$ with respect to any basis is diagonal. \\\\
    Inductive Step: \\
    Let $m > 1$ and suppose that the result holds 
    for all $k < m$.
    Let $U = \text{range }(T - \lambda_m I)$.
    Then, $U$ is invariant under $T$.
    Let $T_U$ be the restriction of $T$ to $U$.
    Then, the minimal polynomial of $T_U$ divides $p(z)/(z - \lambda_m)$.
    Then, the minimal polynomial of $T_U$ has the form
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_{m-1})$$
    By the inductive hypothesis, $T_U$ is diagonalizable and so $U$ has a basis 
    $u_1, \cdots u_m$ consistings of eigenvectors of $T_U$.
    Now, let $u \in \range(T - \lambda_m I) \cap \text{null }(T - \lambda_m I)$ be arbitrary.
    Then, we have
    $Tu = \lambda_m u$ and so 
    $$(T - \lambda_1 I)\cdots (T - \lambda_{m-1})u = 
    (\lambda_m - \lambda_1) \cdots (\lambda_m - \lambda_{m-1})u = 0 $$
    Since the eigenvalues are distinct, we must have $u = 0$.
    Therefore, $\range(T - \lambda_m I) \cap \text{null }(T - \lambda_m I) = \{0\}$.
    and $U + \text{null }(T - \lambda_m I)$ is a direct sum.
    Let $w_1, \cdots w_n$ be a basis of $\text{null }(T - \lambda_m I)$.
    Then,
    $$v_1, \cdots v_m, w_1, \cdots w_n$$
    is linearly independent. Finally, 
    $$m + n = \dim \range(T - \lambda_m I) + \dim \text{null }(T - \lambda_m I) = \dim \V$$
    Therefore, $\V$ has a basis of eigenvectors of $T$ and 
    so $T$ is diagonalizable.
\end{proof}
\begin{theorem}
    Suppose $T$ is diagonalizable and $U$ is a subspace 
    of $\V$ invariant under $T$.
    Then, the restriction of $T$ to $U$ is diagonalizable.
\end{theorem}
\begin{proof}
    The minimal polynomial of $T_U$ divides the minimal polynomial of $T$.
    Since the minimal polynomial of $T$ has the form
    $$(z - \lambda_1)(z - \lambda_2) \cdots (z - \lambda_m)$$
    where $\lambda_1, \cdots \lambda_m$ are distinct eigenvalues of $T$,
    the minimal polynomial of $T_U$ also has this form.
    Therefore, $T_U$ is diagonalizable.
\end{proof}
\section{Inner Product Spaces}
We define the norm of $x = (x_1, x_2, \cdots x_n) \in \mathbb{F}^n$ as
$$\|x\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$$
This is called the Euclidean norm.
We define the dot product 
of $x = (x_1, x_2, \cdots x_n)$ and
$y = (y_1, y_2, \cdots y_n)$ in $\mathbb{F}^n$ as
$$x \cdot y = x_1y_1 + x_2y_2 + \cdots + x_ny_n$$
The dot product satisfies the following properties:
\begin{itemize}
    \item $\| x \|^2 = x \cdot x$.
    \item $x \cdot x \geq 0$ with equality if and only if $x = 0$.
    \item $x \cdot y = y \cdot x$.
    \item $(ax + by) \cdot z = a(x \cdot z) + b(y \cdot z)$ for all $a, b \in \mathbb{F}$.
\end{itemize}
\end{document}